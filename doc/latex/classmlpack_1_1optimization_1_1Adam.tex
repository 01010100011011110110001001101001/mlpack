\section{mlpack\+:\+:optimization\+:\+:Adam$<$ Decomposable\+Function\+Type $>$ Class Template Reference}
\label{classmlpack_1_1optimization_1_1Adam}\index{mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$@{mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$}}


\doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\bf Adam} (Decomposable\+Function\+Type \&{\bf function}, const double {\bf step\+Size}=0.\+001, const double {\bf beta1}=0.\+9, const double {\bf beta2}=0.\+999, const double {\bf eps}=1e-\/8, const size\+\_\+t max\+Iterations=100000, const double tolerance=1e-\/5, const bool shuffle=true, const bool ada\+Max=false)
\begin{DoxyCompactList}\small\item\em Construct the \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} optimizer with the given function and parameters. \end{DoxyCompactList}\item 
bool {\bf Ada\+Max} () const 
\begin{DoxyCompactList}\small\item\em Get whether or not the Ada\+Max optimizer is specified. \end{DoxyCompactList}\item 
bool \& {\bf Ada\+Max} ()
\begin{DoxyCompactList}\small\item\em Modify wehther or not the Ada\+Max optimizer is to be used. \end{DoxyCompactList}\item 
double {\bf Beta1} () const 
\begin{DoxyCompactList}\small\item\em Get the smoothing parameter. \end{DoxyCompactList}\item 
double \& {\bf Beta1} ()
\begin{DoxyCompactList}\small\item\em Modify the smoothing parameter. \end{DoxyCompactList}\item 
double {\bf Beta2} () const 
\begin{DoxyCompactList}\small\item\em Get the second moment coefficient. \end{DoxyCompactList}\item 
double \& {\bf Beta2} ()
\begin{DoxyCompactList}\small\item\em Modify the second moment coefficient. \end{DoxyCompactList}\item 
double {\bf Epsilon} () const 
\begin{DoxyCompactList}\small\item\em Get the value used to initialise the mean squared gradient parameter. \end{DoxyCompactList}\item 
double \& {\bf Epsilon} ()
\begin{DoxyCompactList}\small\item\em Modify the value used to initialise the mean squared gradient parameter. \end{DoxyCompactList}\item 
const Decomposable\+Function\+Type \& {\bf Function} () const 
\begin{DoxyCompactList}\small\item\em Get the instantiated function to be optimized. \end{DoxyCompactList}\item 
Decomposable\+Function\+Type \& {\bf Function} ()
\begin{DoxyCompactList}\small\item\em Modify the instantiated function. \end{DoxyCompactList}\item 
size\+\_\+t {\bf Max\+Iterations} () const 
\begin{DoxyCompactList}\small\item\em Get the maximum number of iterations (0 indicates no limit). \end{DoxyCompactList}\item 
size\+\_\+t \& {\bf Max\+Iterations} ()
\begin{DoxyCompactList}\small\item\em Modify the maximum number of iterations (0 indicates no limit). \end{DoxyCompactList}\item 
double {\bf Optimize} (arma\+::mat \&iterate)
\begin{DoxyCompactList}\small\item\em Optimize the given function using \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam}. \end{DoxyCompactList}\item 
bool {\bf Shuffle} () const 
\begin{DoxyCompactList}\small\item\em Get whether or not the individual functions are shuffled. \end{DoxyCompactList}\item 
bool \& {\bf Shuffle} ()
\begin{DoxyCompactList}\small\item\em Modify whether or not the individual functions are shuffled. \end{DoxyCompactList}\item 
double {\bf Step\+Size} () const 
\begin{DoxyCompactList}\small\item\em Get the step size. \end{DoxyCompactList}\item 
double \& {\bf Step\+Size} ()
\begin{DoxyCompactList}\small\item\em Modify the step size. \end{DoxyCompactList}\item 
double {\bf Tolerance} () const 
\begin{DoxyCompactList}\small\item\em Get the tolerance for termination. \end{DoxyCompactList}\item 
double \& {\bf Tolerance} ()
\begin{DoxyCompactList}\small\item\em Modify the tolerance for termination. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Attributes}
\begin{DoxyCompactItemize}
\item 
bool {\bf ada\+Max}
\begin{DoxyCompactList}\small\item\em Specifies whether or not the Ada\+Max optimizer is to be used. \end{DoxyCompactList}\item 
double {\bf beta1}
\begin{DoxyCompactList}\small\item\em Exponential decay rate for the first moment estimates. \end{DoxyCompactList}\item 
double {\bf beta2}
\begin{DoxyCompactList}\small\item\em Exponential decay rate for the weighted infinity norm estimates. \end{DoxyCompactList}\item 
double {\bf eps}
\begin{DoxyCompactList}\small\item\em The value used to initialise the mean squared gradient parameter. \end{DoxyCompactList}\item 
Decomposable\+Function\+Type \& {\bf function}
\begin{DoxyCompactList}\small\item\em The instantiated function. \end{DoxyCompactList}\item 
size\+\_\+t {\bf max\+Iterations}
\begin{DoxyCompactList}\small\item\em The maximum number of allowed iterations. \end{DoxyCompactList}\item 
bool {\bf shuffle}
\begin{DoxyCompactList}\small\item\em Controls whether or not the individual functions are shuffled when iterating. \end{DoxyCompactList}\item 
double {\bf step\+Size}
\begin{DoxyCompactList}\small\item\em The step size for each example. \end{DoxyCompactList}\item 
double {\bf tolerance}
\begin{DoxyCompactList}\small\item\em The tolerance for termination. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Decomposable\+Function\+Type$>$\\*
class mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$}

\doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. 

Ada\+Max is a variant of \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} based on the infinity norm as given in the section 7 of the following paper.

For more information, see the following.


\begin{DoxyCode}
@article\{Kingma2014,
  author    = \{Diederik P. Kingma and Jimmy Ba\},
  title     = \{Adam: \{A\} Method \textcolor{keywordflow}{for} Stochastic Optimization\},
  journal   = \{CoRR\},
  year      = \{2014\}
\}
\end{DoxyCode}


For \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} and Ada\+Max to work, a Decomposable\+Function\+Type template parameter is required. This class must implement the following function\+:

size\+\_\+t Num\+Functions(); double Evaluate(const arma\+::mat\& coordinates, const size\+\_\+t i); void Gradient(const arma\+::mat\& coordinates, const size\+\_\+t i, arma\+::mat\& gradient);

Num\+Functions() should return the number of functions ( $n$), and in the other two functions, the parameter i refers to which individual function (or gradient) is being evaluated. So, for the case of a data-\/dependent function, such as N\+CA (see \doxyref{mlpack\+::nca\+::\+N\+CA}{p.}{classmlpack_1_1nca_1_1NCA}), Num\+Functions() should return the number of points in the dataset, and Evaluate(coordinates, 0) will evaluate the objective function on the first point in the dataset (presumably, the dataset is held internally in the Decomposable\+Function\+Type).


\begin{DoxyTemplParams}{Template Parameters}
{\em Decomposable\+Function\+Type} & Decomposable objective function type to be minimized. \\
\hline
\end{DoxyTemplParams}


Definition at line 65 of file adam.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Adam@{Adam}}
\index{Adam@{Adam}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Adam(\+Decomposable\+Function\+Type \&function, const double step\+Size=0.\+001, const double beta1=0.\+9, const double beta2=0.\+999, const double eps=1e-\/8, const size\+\_\+t max\+Iterations=100000, const double tolerance=1e-\/5, const bool shuffle=true, const bool ada\+Max=false)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::{\bf Adam} (
\begin{DoxyParamCaption}
\item[{Decomposable\+Function\+Type \&}]{function, }
\item[{const double}]{step\+Size = {\ttfamily 0.001}, }
\item[{const double}]{beta1 = {\ttfamily 0.9}, }
\item[{const double}]{beta2 = {\ttfamily 0.999}, }
\item[{const double}]{eps = {\ttfamily 1e-\/8}, }
\item[{const size\+\_\+t}]{max\+Iterations = {\ttfamily 100000}, }
\item[{const double}]{tolerance = {\ttfamily 1e-\/5}, }
\item[{const bool}]{shuffle = {\ttfamily true}, }
\item[{const bool}]{ada\+Max = {\ttfamily false}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1optimization_1_1Adam_a505d3a8f04c5f59677b456a29f5921c9}


Construct the \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} optimizer with the given function and parameters. 

The defaults here are not necessarily good for the given problem, so it is suggested that the values used be tailored to the task at hand. The maximum number of iterations refers to the maximum number of points that are processed (i.\+e., one iteration equals one point; one iteration does not equal one pass over the dataset).


\begin{DoxyParams}{Parameters}
{\em function} & Function to be optimized (minimized). \\
\hline
{\em step\+Size} & Step size for each iteration. \\
\hline
{\em beta1} & Exponential decay rate for the first moment estimates. \\
\hline
{\em beta2} & Exponential decay rate for the weighted infinity norm estimates. \\
\hline
{\em eps} & Value used to initialise the mean squared gradient parameter. \\
\hline
{\em max\+Iterations} & Maximum number of iterations allowed (0 means no limit). \\
\hline
{\em tolerance} & Maximum absolute tolerance to terminate algorithm. \\
\hline
{\em shuffle} & If true, the function order is shuffled; otherwise, each function is visited in linear order. \\
\hline
{\em ada\+Max} & If true, then the Ada\+Max optimizer is used; otherwise, by default the \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} optimizer is used. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Ada\+Max@{Ada\+Max}}
\index{Ada\+Max@{Ada\+Max}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Ada\+Max() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Ada\+Max (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a997272dbb67581475e614e6bd9b8da61}


Get whether or not the Ada\+Max optimizer is specified. 



Definition at line 151 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::ada\+Max.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Ada\+Max@{Ada\+Max}}
\index{Ada\+Max@{Ada\+Max}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Ada\+Max()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Ada\+Max (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_ab88d5a769f447ac07bc298b30586179a}


Modify wehther or not the Ada\+Max optimizer is to be used. 



Definition at line 153 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::ada\+Max.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Beta1@{Beta1}}
\index{Beta1@{Beta1}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Beta1() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Beta1 (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_ad538e121215045522d3e502832e0e073}


Get the smoothing parameter. 



Definition at line 121 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::beta1.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Beta1@{Beta1}}
\index{Beta1@{Beta1}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Beta1()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Beta1 (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_ad20f27515ccd3de39787edb97c05f165}


Modify the smoothing parameter. 



Definition at line 123 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::beta1.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Beta2@{Beta2}}
\index{Beta2@{Beta2}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Beta2() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Beta2 (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a369459b40c453cd8f9f1162361e9d70c}


Get the second moment coefficient. 



Definition at line 126 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::beta2.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Beta2@{Beta2}}
\index{Beta2@{Beta2}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Beta2()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Beta2 (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_abeaaf93a97f99eb8c7e73f24764e74cc}


Modify the second moment coefficient. 



Definition at line 128 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::beta2.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Epsilon@{Epsilon}}
\index{Epsilon@{Epsilon}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Epsilon() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Epsilon (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a71178e9a83c6a2bfebd29037cf9a377b}


Get the value used to initialise the mean squared gradient parameter. 



Definition at line 131 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::eps.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Epsilon@{Epsilon}}
\index{Epsilon@{Epsilon}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Epsilon()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Epsilon (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a5383ad9795720cf9fe216c109055e835}


Modify the value used to initialise the mean squared gradient parameter. 



Definition at line 133 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::eps.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Function@{Function}}
\index{Function@{Function}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Function() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ const Decomposable\+Function\+Type\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Function (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a27a671448a1a2ed681ca31746686c3c8}


Get the instantiated function to be optimized. 



Definition at line 111 of file adam.\+hpp.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Function@{Function}}
\index{Function@{Function}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Function()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ Decomposable\+Function\+Type\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Function (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a0d72d275a978a23cb4c9a5f556ce7a8e}


Modify the instantiated function. 



Definition at line 113 of file adam.\+hpp.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Max\+Iterations@{Max\+Iterations}}
\index{Max\+Iterations@{Max\+Iterations}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Max\+Iterations() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ size\+\_\+t {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Max\+Iterations (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a2c6089a426b12a338dfd1578db148b86}


Get the maximum number of iterations (0 indicates no limit). 



Definition at line 136 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::max\+Iterations.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Max\+Iterations@{Max\+Iterations}}
\index{Max\+Iterations@{Max\+Iterations}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Max\+Iterations()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ size\+\_\+t\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Max\+Iterations (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_af65e3e290bbf7ee1c3b95b119e2f9bea}


Modify the maximum number of iterations (0 indicates no limit). 



Definition at line 138 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::max\+Iterations.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Optimize@{Optimize}}
\index{Optimize@{Optimize}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Optimize(arma\+::mat \&iterate)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Optimize (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&}]{iterate}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1optimization_1_1Adam_a7c378485d3abc4570a4ff99071c03bbd}


Optimize the given function using \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam}. 

The given starting point will be modified to store the finishing point of the algorithm, and the final objective value is returned.


\begin{DoxyParams}{Parameters}
{\em iterate} & Starting point (will be modified). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Objective value of the final point. 
\end{DoxyReturn}
\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Shuffle@{Shuffle}}
\index{Shuffle@{Shuffle}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Shuffle() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Shuffle (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_aa31296e322e6041a8b3496f783734c79}


Get whether or not the individual functions are shuffled. 



Definition at line 146 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::shuffle.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Shuffle@{Shuffle}}
\index{Shuffle@{Shuffle}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Shuffle()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Shuffle (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_aacf9ce0ffd3c4176daba05e7819864ea}


Modify whether or not the individual functions are shuffled. 



Definition at line 148 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::shuffle.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Step\+Size@{Step\+Size}}
\index{Step\+Size@{Step\+Size}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Step\+Size() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Step\+Size (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a4f3945c5c625d4979a7cf1f31561867b}


Get the step size. 



Definition at line 116 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::step\+Size.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Step\+Size@{Step\+Size}}
\index{Step\+Size@{Step\+Size}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Step\+Size()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Step\+Size (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_aede55fc25b025637bc414dc679b1a576}


Modify the step size. 



Definition at line 118 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::step\+Size.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Tolerance@{Tolerance}}
\index{Tolerance@{Tolerance}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Tolerance() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Tolerance (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a8eda9c96d536058a2e64831e9ff91e72}


Get the tolerance for termination. 



Definition at line 141 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::tolerance.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!Tolerance@{Tolerance}}
\index{Tolerance@{Tolerance}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{Tolerance()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::Tolerance (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1Adam_a87ec9ebf00a64f81c2f4a2fe76e17648}


Modify the tolerance for termination. 



Definition at line 143 of file adam.\+hpp.



References mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::tolerance.



\subsection{Member Data Documentation}
\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!ada\+Max@{ada\+Max}}
\index{ada\+Max@{ada\+Max}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{ada\+Max}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::ada\+Max\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_affc1110c741360c3be99cf150058fb6d}


Specifies whether or not the Ada\+Max optimizer is to be used. 



Definition at line 182 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Ada\+Max().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!beta1@{beta1}}
\index{beta1@{beta1}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{beta1}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::beta1\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_afa2dd9b21d0942283d5243ce1d319b70}


Exponential decay rate for the first moment estimates. 



Definition at line 163 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Beta1().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!beta2@{beta2}}
\index{beta2@{beta2}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{beta2}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::beta2\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_adef01fba112e464a7d1d8e92a81aaadb}


Exponential decay rate for the weighted infinity norm estimates. 



Definition at line 166 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Beta2().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!eps@{eps}}
\index{eps@{eps}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{eps}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::eps\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_a034aa918d9a0b60c392ceecaea2f7ef8}


The value used to initialise the mean squared gradient parameter. 



Definition at line 169 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Epsilon().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!function@{function}}
\index{function@{function}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{function}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ Decomposable\+Function\+Type\& {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::function\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_af308f2c8a08bdfe130d0c6fd6a1f7b67}


The instantiated function. 



Definition at line 157 of file adam.\+hpp.

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!max\+Iterations@{max\+Iterations}}
\index{max\+Iterations@{max\+Iterations}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{max\+Iterations}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ size\+\_\+t {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::max\+Iterations\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_aa787186bf9e208864c265f771b142450}


The maximum number of allowed iterations. 



Definition at line 172 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Max\+Iterations().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!shuffle@{shuffle}}
\index{shuffle@{shuffle}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{shuffle}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::shuffle\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_a10d379c675624ea5fcfed3990469f227}


Controls whether or not the individual functions are shuffled when iterating. 



Definition at line 179 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Shuffle().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!step\+Size@{step\+Size}}
\index{step\+Size@{step\+Size}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{step\+Size}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::step\+Size\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_a78eb21d8bb4f31a559635f881b88c3af}


The step size for each example. 



Definition at line 160 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Step\+Size().

\index{mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}!tolerance@{tolerance}}
\index{tolerance@{tolerance}!mlpack\+::optimization\+::\+Adam@{mlpack\+::optimization\+::\+Adam}}
\subsubsection[{tolerance}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Adam}$<$ Decomposable\+Function\+Type $>$\+::tolerance\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1Adam_a5e2bb73d504950407ecdc5780cdaff0d}


The tolerance for termination. 



Definition at line 175 of file adam.\+hpp.



Referenced by mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$\+::\+Tolerance().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/mlpack/core/optimizers/adam/{\bf adam.\+hpp}\end{DoxyCompactItemize}
