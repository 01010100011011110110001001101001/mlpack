\section{mlpack\+:\+:ann\+:\+:F\+FN$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1FFN}\index{mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$@{mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$}}


Implementation of a standard feed forward network.  


\subsection*{Public Types}
\begin{DoxyCompactItemize}
\item 
using {\bf Network\+Type} = {\bf F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$
\begin{DoxyCompactList}\small\item\em Convenience typedef for the internal model construction. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\bf F\+FN} (Output\+Layer\+Type \&\&{\bf output\+Layer}=Output\+Layer\+Type(), Initialization\+Rule\+Type {\bf initialize\+Rule}=Initialization\+Rule\+Type())
\begin{DoxyCompactList}\small\item\em Create the \doxyref{F\+FN}{p.}{classmlpack_1_1ann_1_1FFN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. \end{DoxyCompactList}\item 
{\bf F\+FN} (const arma\+::mat \&{\bf predictors}, const arma\+::mat \&{\bf responses}, Output\+Layer\+Type \&\&{\bf output\+Layer}=Output\+Layer\+Type(), Initialization\+Rule\+Type {\bf initialize\+Rule}=Initialization\+Rule\+Type())
\begin{DoxyCompactList}\small\item\em Create the \doxyref{F\+FN}{p.}{classmlpack_1_1ann_1_1FFN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. \end{DoxyCompactList}\item 
{\bf $\sim$\+F\+FN} ()
\begin{DoxyCompactList}\small\item\em Destructor to release allocated memory. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Layer\+Type , class... Args$>$ }\\void {\bf Add} (Args...\+args)
\item 
void {\bf Add} ({\bf Layer\+Types} layer)
\item 
double {\bf Evaluate} (const arma\+::mat \&parameters, const size\+\_\+t i, const bool {\bf deterministic}=true)
\begin{DoxyCompactList}\small\item\em Evaluate the feedforward network with the given parameters. \end{DoxyCompactList}\item 
void {\bf Gradient} (const arma\+::mat \&parameters, const size\+\_\+t i, arma\+::mat \&{\bf gradient})
\begin{DoxyCompactList}\small\item\em Evaluate the gradient of the feedforward network with the given parameters, and with respect to only one point in the dataset. \end{DoxyCompactList}\item 
size\+\_\+t {\bf Num\+Functions} () const 
\begin{DoxyCompactList}\small\item\em Return the number of separable functions (the number of predictor points). \end{DoxyCompactList}\item 
const arma\+::mat \& {\bf Parameters} () const 
\begin{DoxyCompactList}\small\item\em Return the initial point for the optimization. \end{DoxyCompactList}\item 
arma\+::mat \& {\bf Parameters} ()
\begin{DoxyCompactList}\small\item\em Modify the initial point for the optimization. \end{DoxyCompactList}\item 
void {\bf Predict} (arma\+::mat \&{\bf predictors}, arma\+::mat \&{\bf responses})
\begin{DoxyCompactList}\small\item\em Predict the responses to a given set of predictors. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void {\bf Serialize} (Archive \&ar, const unsigned int)
\begin{DoxyCompactList}\small\item\em Serialize the model. \end{DoxyCompactList}\item 
{\footnotesize template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+R\+M\+Sprop$>$ }\\void {\bf Train} (const arma\+::mat \&{\bf predictors}, const arma\+::mat \&{\bf responses}, Optimizer\+Type$<$ {\bf Network\+Type} $>$ \&optimizer)
\begin{DoxyCompactList}\small\item\em Train the feedforward network on the given input data using the given optimizer. \end{DoxyCompactList}\item 
{\footnotesize template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+R\+M\+Sprop$>$ }\\void {\bf Train} (const arma\+::mat \&{\bf predictors}, const arma\+::mat \&{\bf responses})
\begin{DoxyCompactList}\small\item\em Train the feedforward network on the given input data. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Member Functions}
\begin{DoxyCompactItemize}
\item 
void {\bf Backward} ()
\begin{DoxyCompactList}\small\item\em The Backward algorithm (part of the Forward-\/\+Backward algorithm). \end{DoxyCompactList}\item 
void {\bf Forward} (arma\+::mat \&\&input)
\begin{DoxyCompactList}\small\item\em The Forward algorithm (part of the Forward-\/\+Backward algorithm). \end{DoxyCompactList}\item 
void {\bf Gradient} ()
\begin{DoxyCompactList}\small\item\em Iterate through all layer modules and update the the gradient using the layer defined optimizer. \end{DoxyCompactList}\item 
void {\bf Reset\+Deterministic} ()
\begin{DoxyCompactList}\small\item\em Reset the module status by setting the current deterministic parameter for all modules that implement the Deterministic function. \end{DoxyCompactList}\item 
void {\bf Reset\+Gradients} (arma\+::mat \&{\bf gradient})
\begin{DoxyCompactList}\small\item\em Reset the gradient for all modules that implement the Gradient function. \end{DoxyCompactList}\item 
void {\bf Reset\+Parameters} ()
\begin{DoxyCompactList}\small\item\em Reset the module infomration (weights/parameters). \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Attributes}
\begin{DoxyCompactItemize}
\item 
arma\+::mat {\bf current\+Input}
\begin{DoxyCompactList}\small\item\em T\+He current input of the forward/backward pass. \end{DoxyCompactList}\item 
arma\+::mat {\bf current\+Target}
\begin{DoxyCompactList}\small\item\em T\+He current target of the forward/backward pass. \end{DoxyCompactList}\item 
{\bf Delete\+Visitor} {\bf delete\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delete visitor. \end{DoxyCompactList}\item 
arma\+::mat {\bf delta}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delta object. \end{DoxyCompactList}\item 
{\bf Delta\+Visitor} {\bf delta\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delta visitor. \end{DoxyCompactList}\item 
bool {\bf deterministic}
\begin{DoxyCompactList}\small\item\em The current evaluation mode (training or testing). \end{DoxyCompactList}\item 
arma\+::mat {\bf error}
\begin{DoxyCompactList}\small\item\em The current error for the backward pass. \end{DoxyCompactList}\item 
arma\+::mat {\bf gradient}
\begin{DoxyCompactList}\small\item\em Locally-\/stored gradient parameter. \end{DoxyCompactList}\item 
size\+\_\+t {\bf height}
\begin{DoxyCompactList}\small\item\em The input height. \end{DoxyCompactList}\item 
Initialization\+Rule\+Type {\bf initialize\+Rule}
\begin{DoxyCompactList}\small\item\em Instantiated Initialization\+Rule object for initializing the network parameter. \end{DoxyCompactList}\item 
arma\+::mat {\bf input\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored input parameter object. \end{DoxyCompactList}\item 
std\+::vector$<$ {\bf Layer\+Types} $>$ {\bf network}
\begin{DoxyCompactList}\small\item\em Locally-\/stored model modules. \end{DoxyCompactList}\item 
size\+\_\+t {\bf num\+Functions}
\begin{DoxyCompactList}\small\item\em The number of separable functions (the number of predictor points). \end{DoxyCompactList}\item 
{\bf Output\+Height\+Visitor} {\bf output\+Height\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output height visitor. \end{DoxyCompactList}\item 
Output\+Layer\+Type {\bf output\+Layer}
\begin{DoxyCompactList}\small\item\em Instantiated outputlayer used to evaluate the network. \end{DoxyCompactList}\item 
arma\+::mat {\bf output\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output parameter object. \end{DoxyCompactList}\item 
{\bf Output\+Parameter\+Visitor} {\bf output\+Parameter\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output parameter visitor. \end{DoxyCompactList}\item 
{\bf Output\+Width\+Visitor} {\bf output\+Width\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output width visitor. \end{DoxyCompactList}\item 
arma\+::mat {\bf parameter}
\begin{DoxyCompactList}\small\item\em Matrix of (trained) parameters. \end{DoxyCompactList}\item 
arma\+::mat {\bf predictors}
\begin{DoxyCompactList}\small\item\em The matrix of data points (predictors). \end{DoxyCompactList}\item 
bool {\bf reset}
\begin{DoxyCompactList}\small\item\em Indicator if we already trained the model. \end{DoxyCompactList}\item 
{\bf Reset\+Visitor} {\bf reset\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored reset visitor. \end{DoxyCompactList}\item 
arma\+::mat {\bf responses}
\begin{DoxyCompactList}\small\item\em The matrix of responses to the input data points. \end{DoxyCompactList}\item 
{\bf Weight\+Size\+Visitor} {\bf weight\+Size\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored weight size visitor. \end{DoxyCompactList}\item 
size\+\_\+t {\bf width}
\begin{DoxyCompactList}\small\item\em The input width. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Output\+Layer\+Type = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type = Random\+Initialization$>$\\*
class mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$}

Implementation of a standard feed forward network. 


\begin{DoxyTemplParams}{Template Parameters}
{\em Output\+Layer\+Type} & The output layer type used to evaluate the network. \\
\hline
{\em Initialization\+Rule\+Type} & Rule used to initialize the weight matrix. \\
\hline
\end{DoxyTemplParams}


Definition at line 42 of file ffn.\+hpp.



\subsection{Member Typedef Documentation}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Network\+Type@{Network\+Type}}
\index{Network\+Type@{Network\+Type}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Network\+Type}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ using {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Network\+Type} =  {\bf F\+FN}$<$Output\+Layer\+Type, Initialization\+Rule\+Type$>$}\label{classmlpack_1_1ann_1_1FFN_ab6b3d2b9031b6366391dc465df821a5b}


Convenience typedef for the internal model construction. 



Definition at line 46 of file ffn.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!F\+FN@{F\+FN}}
\index{F\+FN@{F\+FN}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{F\+F\+N(\+Output\+Layer\+Type \&\&output\+Layer=\+Output\+Layer\+Type(), Initialization\+Rule\+Type initialize\+Rule=\+Initialization\+Rule\+Type())}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf F\+FN} (
\begin{DoxyParamCaption}
\item[{Output\+Layer\+Type \&\&}]{output\+Layer = {\ttfamily OutputLayerType()}, }
\item[{Initialization\+Rule\+Type}]{initialize\+Rule = {\ttfamily InitializationRuleType()}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_a6639df2b31755cfd50baa468125a5301}


Create the \doxyref{F\+FN}{p.}{classmlpack_1_1ann_1_1FFN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. 

Optionally, specify which initialize rule and performance function should be used.


\begin{DoxyParams}{Parameters}
{\em output\+Layer} & Output layer used to evaluate the network. \\
\hline
{\em initialize\+Rule} & Optional instantiated Initialization\+Rule object for initializing the network parameter. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!F\+FN@{F\+FN}}
\index{F\+FN@{F\+FN}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{F\+F\+N(const arma\+::mat \&predictors, const arma\+::mat \&responses, Output\+Layer\+Type \&\&output\+Layer=\+Output\+Layer\+Type(), Initialization\+Rule\+Type initialize\+Rule=\+Initialization\+Rule\+Type())}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf F\+FN} (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{const arma\+::mat \&}]{responses, }
\item[{Output\+Layer\+Type \&\&}]{output\+Layer = {\ttfamily OutputLayerType()}, }
\item[{Initialization\+Rule\+Type}]{initialize\+Rule = {\ttfamily InitializationRuleType()}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_a4790cbe5fb33c5192548c042a6ec5a83}


Create the \doxyref{F\+FN}{p.}{classmlpack_1_1ann_1_1FFN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. 

Optionally, specify which initialize rule and performance function should be used.


\begin{DoxyParams}{Parameters}
{\em predictors} & Input training variables. \\
\hline
{\em responses} & Outputs results from input training variables. \\
\hline
{\em output\+Layer} & Output layer used to evaluate the network. \\
\hline
{\em initialize\+Rule} & Optional instantiated Initialization\+Rule object for initializing the network parameter. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!````~F\+FN@{$\sim$\+F\+FN}}
\index{````~F\+FN@{$\sim$\+F\+FN}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{$\sim$\+F\+F\+N()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::$\sim${\bf F\+FN} (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_a6068e572988764aa7b327c8161462618}


Destructor to release allocated memory. 



\subsection{Member Function Documentation}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Add(\+Args...\+args)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$class Layer\+Type , class... Args$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{Args...}]{args}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1FFN_ad8d753d4b2915592f1e180ade35714d1}


Definition at line 161 of file ffn.\+hpp.



References mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Add(\+Layer\+Types layer)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{{\bf Layer\+Types}}]{layer}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1FFN_a4dd88455623b2bc7e02773d993ac904f}


Definition at line 168 of file ffn.\+hpp.



References mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Backward()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Backward (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a38459a33cc44083aae107e7541652745}


The Backward algorithm (part of the Forward-\/\+Backward algorithm). 

Computes backward pass for module. 

Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Evaluate@{Evaluate}}
\index{Evaluate@{Evaluate}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Evaluate(const arma\+::mat \&parameters, const size\+\_\+t i, const bool deterministic=true)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ double {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Evaluate (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{parameters, }
\item[{const size\+\_\+t}]{i, }
\item[{const bool}]{deterministic = {\ttfamily true}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_ad9895fbe59fa742793b1ef30eeb516be}


Evaluate the feedforward network with the given parameters. 

This function is usually called by the optimizer to train the model.


\begin{DoxyParams}{Parameters}
{\em parameters} & Matrix model parameters. \\
\hline
{\em i} & Index of point to use for objective function evaluation. \\
\hline
{\em deterministic} & Whether or not to train or test the model. Note some layer act differently in training or testing mode. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Forward(arma\+::mat \&\&input)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Forward (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&\&}]{input}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a74effb0ad89f09cfdf196288be826793}


The Forward algorithm (part of the Forward-\/\+Backward algorithm). 

Computes forward probabilities for each module.


\begin{DoxyParams}{Parameters}
{\em input} & Data sequence to compute probabilities for. \\
\hline
\end{DoxyParams}


Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Gradient(const arma\+::mat \&parameters, const size\+\_\+t i, arma\+::mat \&gradient)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{parameters, }
\item[{const size\+\_\+t}]{i, }
\item[{arma\+::mat \&}]{gradient}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_a8536a112302755d1acedb61ccaa25c89}


Evaluate the gradient of the feedforward network with the given parameters, and with respect to only one point in the dataset. 

This is useful for optimizers such as S\+GD, which require a separable objective function.


\begin{DoxyParams}{Parameters}
{\em parameters} & Matrix of the model parameters to be optimized. \\
\hline
{\em i} & Index of points to use for objective function gradient evaluation. \\
\hline
{\em gradient} & Matrix to output gradient into. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Gradient()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_afc3ece828a6bbaa703ed377bb53c02e4}


Iterate through all layer modules and update the the gradient using the layer defined optimizer. 



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Num\+Functions@{Num\+Functions}}
\index{Num\+Functions@{Num\+Functions}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Num\+Functions() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Num\+Functions (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1FFN_af2016d567c1d7dce58ed2183fc2578d4}


Return the number of separable functions (the number of predictor points). 



Definition at line 171 of file ffn.\+hpp.



References mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::num\+Functions.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Parameters() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ const arma\+::mat\& {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1FFN_a11dbfacb6026a8898625f0c69fd6f635}


Return the initial point for the optimization. 



Definition at line 174 of file ffn.\+hpp.



References mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::parameter.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Parameters()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat\& {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1FFN_a828f79e7d259fb979b4b10b35e100567}


Modify the initial point for the optimization. 



Definition at line 176 of file ffn.\+hpp.



References mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Backward(), mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Forward(), mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Gradient(), mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::gradient, mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::parameter, mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Reset\+Deterministic(), mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Reset\+Gradients(), mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Reset\+Parameters(), and mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Serialize().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Predict@{Predict}}
\index{Predict@{Predict}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Predict(arma\+::mat \&predictors, arma\+::mat \&responses)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Predict (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&}]{predictors, }
\item[{arma\+::mat \&}]{responses}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_aa6e9105a74a9569d9248712ea2a70dc8}


Predict the responses to a given set of predictors. 

The responses will reflect the output of the given output layer as returned by the output layer function.


\begin{DoxyParams}{Parameters}
{\em predictors} & Input predictors. \\
\hline
{\em responses} & Matrix to put output predictions of responses into. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Reset\+Deterministic@{Reset\+Deterministic}}
\index{Reset\+Deterministic@{Reset\+Deterministic}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Reset\+Deterministic()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Reset\+Deterministic (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_ac9cff0fca2334a489b51b8f979cb54c3}


Reset the module status by setting the current deterministic parameter for all modules that implement the Deterministic function. 



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Reset\+Gradients@{Reset\+Gradients}}
\index{Reset\+Gradients@{Reset\+Gradients}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Reset\+Gradients(arma\+::mat \&gradient)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Reset\+Gradients (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&}]{gradient}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a4ca578ce1277ef3c048ecf3709f49b44}


Reset the gradient for all modules that implement the Gradient function. 



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Reset\+Parameters@{Reset\+Parameters}}
\index{Reset\+Parameters@{Reset\+Parameters}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Reset\+Parameters()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Reset\+Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a5f55142a4bc136653bc9413db3eb35bf}


Reset the module infomration (weights/parameters). 



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Serialize@{Serialize}}
\index{Serialize@{Serialize}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Serialize(\+Archive \&ar, const unsigned int)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$typename Archive $>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Serialize (
\begin{DoxyParamCaption}
\item[{Archive \&}]{ar, }
\item[{const unsigned}]{int}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_a0435312952cd7da8ef016221b464e083}


Serialize the model. 



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Train@{Train}}
\index{Train@{Train}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Train(const arma\+::mat \&predictors, const arma\+::mat \&responses, Optimizer\+Type$<$ Network\+Type $>$ \&optimizer)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+R\+M\+Sprop$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Train (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{const arma\+::mat \&}]{responses, }
\item[{Optimizer\+Type$<$ {\bf Network\+Type} $>$ \&}]{optimizer}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_a4c8ddff761983f9c45b611e92e9e5513}


Train the feedforward network on the given input data using the given optimizer. 

This will use the existing model parameters as a starting point for the optimization. If this is not what you want, then you should access the parameters vector directly with \doxyref{Parameters()}{p.}{classmlpack_1_1ann_1_1FFN_a828f79e7d259fb979b4b10b35e100567} and modify it as desired.


\begin{DoxyTemplParams}{Template Parameters}
{\em Optimizer\+Type} & Type of optimizer to use to train the model. \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em predictors} & Input training variables. \\
\hline
{\em responses} & Outputs results from input training variables. \\
\hline
{\em optimizer} & Instantiated optimizer used to train the model. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!Train@{Train}}
\index{Train@{Train}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{Train(const arma\+::mat \&predictors, const arma\+::mat \&responses)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+R\+M\+Sprop$>$ void {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Train (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{const arma\+::mat \&}]{responses}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1FFN_ac10bd80c7d048faa253873846a2d5464}


Train the feedforward network on the given input data. 

By default, the R\+M\+Sprop optimization algorithm is used, but others can be specified (such as \doxyref{mlpack\+::optimization\+::\+S\+GD}{p.}{classmlpack_1_1optimization_1_1SGD}).

This will use the existing model parameters as a starting point for the optimization. If this is not what you want, then you should access the parameters vector directly with \doxyref{Parameters()}{p.}{classmlpack_1_1ann_1_1FFN_a828f79e7d259fb979b4b10b35e100567} and modify it as desired.


\begin{DoxyTemplParams}{Template Parameters}
{\em Optimizer\+Type} & Type of optimizer to use to train the model. \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em predictors} & Input training variables. \\
\hline
{\em responses} & Outputs results from input training variables. \\
\hline
\end{DoxyParams}


\subsection{Member Data Documentation}
\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!current\+Input@{current\+Input}}
\index{current\+Input@{current\+Input}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{current\+Input}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::current\+Input\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a888843c1def9fbe1c7f0a22a650e2137}


T\+He current input of the forward/backward pass. 



Definition at line 255 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!current\+Target@{current\+Target}}
\index{current\+Target@{current\+Target}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{current\+Target}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::current\+Target\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a5e0ca57016ce9ee9ebb2324b88cf64b4}


T\+He current target of the forward/backward pass. 



Definition at line 258 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!delete\+Visitor@{delete\+Visitor}}
\index{delete\+Visitor@{delete\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{delete\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Delete\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::delete\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a96ca46b86a523fd8efb0ae05ed2448c4}


Locally-\/stored delete visitor. 



Definition at line 279 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!delta@{delta}}
\index{delta@{delta}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{delta}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::delta\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_af32d083bd507d972a66585a5b898c254}


Locally-\/stored delta object. 



Definition at line 285 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!delta\+Visitor@{delta\+Visitor}}
\index{delta\+Visitor@{delta\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{delta\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Delta\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::delta\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_acdd1ee726cd108d6f0be50890f591b71}


Locally-\/stored delta visitor. 



Definition at line 261 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!deterministic@{deterministic}}
\index{deterministic@{deterministic}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{deterministic}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ bool {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::deterministic\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a68641a8065b7a390f44f734b4c9c8f42}


The current evaluation mode (training or testing). 



Definition at line 282 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!error@{error}}
\index{error@{error}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{error}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::error\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_aa54f5c40a6db5ad632f007c2be1b6d43}


The current error for the backward pass. 



Definition at line 252 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!gradient@{gradient}}
\index{gradient@{gradient}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{gradient}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::gradient\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_af69e291c0a92a467e46f9a2e624e313f}


Locally-\/stored gradient parameter. 



Definition at line 294 of file ffn.\+hpp.



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!height@{height}}
\index{height@{height}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{height}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::height\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a64a31d975881d54e9e6cf33b06b09a78}


The input height. 



Definition at line 231 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!initialize\+Rule@{initialize\+Rule}}
\index{initialize\+Rule@{initialize\+Rule}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{initialize\+Rule}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ Initialization\+Rule\+Type {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::initialize\+Rule\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a1a8ee04477758a92bd1ee54a77df495b}


Instantiated Initialization\+Rule object for initializing the network parameter. 



Definition at line 225 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!input\+Parameter@{input\+Parameter}}
\index{input\+Parameter@{input\+Parameter}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{input\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::input\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_afd816bf971f550f3a27fcec80538715b}


Locally-\/stored input parameter object. 



Definition at line 288 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!network@{network}}
\index{network@{network}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{network}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ std\+::vector$<${\bf Layer\+Types}$>$ {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_af44edb553c2b84e514442dff1359f02b}


Locally-\/stored model modules. 



Definition at line 237 of file ffn.\+hpp.



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Add().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!num\+Functions@{num\+Functions}}
\index{num\+Functions@{num\+Functions}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{num\+Functions}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::num\+Functions\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a0c2e09cf835d216542951a588bc6446a}


The number of separable functions (the number of predictor points). 



Definition at line 249 of file ffn.\+hpp.



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Num\+Functions().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!output\+Height\+Visitor@{output\+Height\+Visitor}}
\index{output\+Height\+Visitor@{output\+Height\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{output\+Height\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Output\+Height\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Height\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a53e3996d06b5c985627e68c1425cea81}


Locally-\/stored output height visitor. 



Definition at line 273 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!output\+Layer@{output\+Layer}}
\index{output\+Layer@{output\+Layer}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{output\+Layer}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ Output\+Layer\+Type {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Layer\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a18fa2d6e67208a3643063bc8f09fd9b1}


Instantiated outputlayer used to evaluate the network. 



Definition at line 221 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!output\+Parameter@{output\+Parameter}}
\index{output\+Parameter@{output\+Parameter}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{output\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_aea70f98d8cd8f9843222432c870712a5}


Locally-\/stored output parameter object. 



Definition at line 291 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!output\+Parameter\+Visitor@{output\+Parameter\+Visitor}}
\index{output\+Parameter\+Visitor@{output\+Parameter\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{output\+Parameter\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Output\+Parameter\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Parameter\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a2ace3824b879f21c9e3ec894fc71413b}


Locally-\/stored output parameter visitor. 



Definition at line 264 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!output\+Width\+Visitor@{output\+Width\+Visitor}}
\index{output\+Width\+Visitor@{output\+Width\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{output\+Width\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Output\+Width\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Width\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a05e73c63693f0fbd9877037caddb2cf9}


Locally-\/stored output width visitor. 



Definition at line 270 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!parameter@{parameter}}
\index{parameter@{parameter}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a897b9e0746ead7ef8cb343f5708b1e73}


Matrix of (trained) parameters. 



Definition at line 246 of file ffn.\+hpp.



Referenced by mlpack\+::ann\+::\+F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!predictors@{predictors}}
\index{predictors@{predictors}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{predictors}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::predictors\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a7cec959386da1b5d36a6f8cd7f7dfa97}


The matrix of data points (predictors). 



Definition at line 240 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!reset@{reset}}
\index{reset@{reset}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{reset}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ bool {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::reset\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a983096ec0cb98659ffadf1c9e7a86b47}


Indicator if we already trained the model. 



Definition at line 234 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!reset\+Visitor@{reset\+Visitor}}
\index{reset\+Visitor@{reset\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{reset\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Reset\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::reset\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a847069b451d950712e69507554eab2f6}


Locally-\/stored reset visitor. 



Definition at line 276 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!responses@{responses}}
\index{responses@{responses}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{responses}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::responses\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a44b255275fc384bec4a18f1d9ed71832}


The matrix of responses to the input data points. 



Definition at line 243 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!weight\+Size\+Visitor@{weight\+Size\+Visitor}}
\index{weight\+Size\+Visitor@{weight\+Size\+Visitor}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{weight\+Size\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Weight\+Size\+Visitor} {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::weight\+Size\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_a966317dffdb264f8e33e66100f776cab}


Locally-\/stored weight size visitor. 



Definition at line 267 of file ffn.\+hpp.

\index{mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}!width@{width}}
\index{width@{width}!mlpack\+::ann\+::\+F\+FN@{mlpack\+::ann\+::\+F\+FN}}
\subsubsection[{width}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+F\+FN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::width\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1FFN_ac6169f7111e2199aca80c243dcd2e124}


The input width. 



Definition at line 228 of file ffn.\+hpp.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/mlpack/methods/ann/{\bf ffn.\+hpp}\end{DoxyCompactItemize}
