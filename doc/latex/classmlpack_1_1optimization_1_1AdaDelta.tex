\section{mlpack\+:\+:optimization\+:\+:Ada\+Delta$<$ Decomposable\+Function\+Type $>$ Class Template Reference}
\label{classmlpack_1_1optimization_1_1AdaDelta}\index{mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$@{mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$}}


Adadelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad method\+:  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\bf Ada\+Delta} (Decomposable\+Function\+Type \&{\bf function}, const double {\bf rho}=0.\+95, const double {\bf eps}=1e-\/6, const size\+\_\+t max\+Iterations=100000, const double tolerance=1e-\/5, const bool shuffle=true)
\begin{DoxyCompactList}\small\item\em Construct the \doxyref{Ada\+Delta}{p.}{classmlpack_1_1optimization_1_1AdaDelta} optimizer with the given function and parameters. \end{DoxyCompactList}\item 
double {\bf Epsilon} () const 
\begin{DoxyCompactList}\small\item\em Get the value used to initialise the mean squared gradient parameter. \end{DoxyCompactList}\item 
double \& {\bf Epsilon} ()
\begin{DoxyCompactList}\small\item\em Modify the value used to initialise the mean squared gradient parameter. \end{DoxyCompactList}\item 
const Decomposable\+Function\+Type \& {\bf Function} () const 
\begin{DoxyCompactList}\small\item\em Get the instantiated function to be optimized. \end{DoxyCompactList}\item 
Decomposable\+Function\+Type \& {\bf Function} ()
\begin{DoxyCompactList}\small\item\em Modify the instantiated function. \end{DoxyCompactList}\item 
size\+\_\+t {\bf Max\+Iterations} () const 
\begin{DoxyCompactList}\small\item\em Get the maximum number of iterations (0 indicates no limit). \end{DoxyCompactList}\item 
size\+\_\+t \& {\bf Max\+Iterations} ()
\begin{DoxyCompactList}\small\item\em Modify the maximum number of iterations (0 indicates no limit). \end{DoxyCompactList}\item 
double {\bf Optimize} (arma\+::mat \&iterate)
\begin{DoxyCompactList}\small\item\em Optimize the given function using \doxyref{Ada\+Delta}{p.}{classmlpack_1_1optimization_1_1AdaDelta}. \end{DoxyCompactList}\item 
double {\bf Rho} () const 
\begin{DoxyCompactList}\small\item\em Get the smoothing parameter. \end{DoxyCompactList}\item 
double \& {\bf Rho} ()
\begin{DoxyCompactList}\small\item\em Modify the smoothing parameter. \end{DoxyCompactList}\item 
bool {\bf Shuffle} () const 
\begin{DoxyCompactList}\small\item\em Get whether or not the individual functions are shuffled. \end{DoxyCompactList}\item 
bool \& {\bf Shuffle} ()
\begin{DoxyCompactList}\small\item\em Modify whether or not the individual functions are shuffled. \end{DoxyCompactList}\item 
double {\bf Tolerance} () const 
\begin{DoxyCompactList}\small\item\em Get the tolerance for termination. \end{DoxyCompactList}\item 
double \& {\bf Tolerance} ()
\begin{DoxyCompactList}\small\item\em Modify the tolerance for termination. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Attributes}
\begin{DoxyCompactItemize}
\item 
double {\bf eps}
\begin{DoxyCompactList}\small\item\em The value used to initialise the mean squared gradient parameter. \end{DoxyCompactList}\item 
Decomposable\+Function\+Type \& {\bf function}
\begin{DoxyCompactList}\small\item\em The instantiated function. \end{DoxyCompactList}\item 
size\+\_\+t {\bf max\+Iterations}
\begin{DoxyCompactList}\small\item\em The maximum number of allowed iterations. \end{DoxyCompactList}\item 
double {\bf rho}
\begin{DoxyCompactList}\small\item\em The smoothing parameter. \end{DoxyCompactList}\item 
bool {\bf shuffle}
\begin{DoxyCompactList}\small\item\em Controls whether or not the individual functions are shuffled when iterating. \end{DoxyCompactList}\item 
double {\bf tolerance}
\begin{DoxyCompactList}\small\item\em The tolerance for termination. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Decomposable\+Function\+Type$>$\\*
class mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$}

Adadelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad method\+: 


\begin{DoxyItemize}
\item Accumulate Over Window
\item Correct Units with Hessian Approximation
\end{DoxyItemize}

For more information, see the following.


\begin{DoxyCode}
@article\{Zeiler2012,
  author    = \{Matthew D. Zeiler\},
  title     = \{\{ADADELTA:\} An Adaptive Learning Rate Method\},
  journal   = \{CoRR\},
  year      = \{2012\}
\}
\end{DoxyCode}


For \doxyref{Ada\+Delta}{p.}{classmlpack_1_1optimization_1_1AdaDelta} to work, a Decomposable\+Function\+Type template parameter is required. This class must implement the following function\+:

size\+\_\+t Num\+Functions(); double Evaluate(const arma\+::mat\& coordinates, const size\+\_\+t i); void Gradient(const arma\+::mat\& coordinates, const size\+\_\+t i, arma\+::mat\& gradient);

Num\+Functions() should return the number of functions ( $n$), and in the other two functions, the parameter i refers to which individual function (or gradient) is being evaluated. So, for the case of a data-\/dependent function, such as N\+CA (see \doxyref{mlpack\+::nca\+::\+N\+CA}{p.}{classmlpack_1_1nca_1_1NCA}), Num\+Functions() should return the number of points in the dataset, and Evaluate(coordinates, 0) will evaluate the objective function on the first point in the dataset (presumably, the dataset is held internally in the Decomposable\+Function\+Type).


\begin{DoxyTemplParams}{Template Parameters}
{\em Decomposable\+Function\+Type} & Decomposable objective function type to be minimized. \\
\hline
\end{DoxyTemplParams}


Definition at line 63 of file ada\+\_\+delta.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Ada\+Delta@{Ada\+Delta}}
\index{Ada\+Delta@{Ada\+Delta}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Ada\+Delta(\+Decomposable\+Function\+Type \&function, const double rho=0.\+95, const double eps=1e-\/6, const size\+\_\+t max\+Iterations=100000, const double tolerance=1e-\/5, const bool shuffle=true)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::{\bf Ada\+Delta} (
\begin{DoxyParamCaption}
\item[{Decomposable\+Function\+Type \&}]{function, }
\item[{const double}]{rho = {\ttfamily 0.95}, }
\item[{const double}]{eps = {\ttfamily 1e-\/6}, }
\item[{const size\+\_\+t}]{max\+Iterations = {\ttfamily 100000}, }
\item[{const double}]{tolerance = {\ttfamily 1e-\/5}, }
\item[{const bool}]{shuffle = {\ttfamily true}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1optimization_1_1AdaDelta_afe555f09f1ddd619ee0b48aeb5a8a321}


Construct the \doxyref{Ada\+Delta}{p.}{classmlpack_1_1optimization_1_1AdaDelta} optimizer with the given function and parameters. 

The defaults here are not necessarily good for the given problem, so it is suggested that the values used be tailored to the task at hand. The maximum number of iterations refers to the maximum number of points that are processed (i.\+e., one iteration equals one point; one iteration does not equal one pass over the dataset).


\begin{DoxyParams}{Parameters}
{\em function} & Function to be optimized (minimized). \\
\hline
{\em rho} & Smoothing constant \\
\hline
{\em eps} & Value used to initialise the mean squared gradient parameter. \\
\hline
{\em max\+Iterations} & Maximum number of iterations allowed (0 means no limit). \\
\hline
{\em tolerance} & Maximum absolute tolerance to terminate algorithm. \\
\hline
{\em shuffle} & If true, the function order is shuffled; otherwise, each function is visited in linear order. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Epsilon@{Epsilon}}
\index{Epsilon@{Epsilon}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Epsilon() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Epsilon (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_ab0e90a059982337e72c439c90c0ccd96}


Get the value used to initialise the mean squared gradient parameter. 



Definition at line 111 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::eps.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Epsilon@{Epsilon}}
\index{Epsilon@{Epsilon}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Epsilon()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Epsilon (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a9c73431710b7b132e04e58c7d41f0082}


Modify the value used to initialise the mean squared gradient parameter. 



Definition at line 113 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::eps.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Function@{Function}}
\index{Function@{Function}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Function() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ const Decomposable\+Function\+Type\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Function (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a0db4ce1195b7edbd8dea8e6746c6612f}


Get the instantiated function to be optimized. 



Definition at line 101 of file ada\+\_\+delta.\+hpp.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Function@{Function}}
\index{Function@{Function}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Function()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ Decomposable\+Function\+Type\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Function (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_ad0205451386948b4b9c09a4d3fed59ad}


Modify the instantiated function. 



Definition at line 103 of file ada\+\_\+delta.\+hpp.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Max\+Iterations@{Max\+Iterations}}
\index{Max\+Iterations@{Max\+Iterations}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Max\+Iterations() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ size\+\_\+t {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Max\+Iterations (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a500124f0dc953ade5965c388106200c4}


Get the maximum number of iterations (0 indicates no limit). 



Definition at line 116 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::max\+Iterations.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Max\+Iterations@{Max\+Iterations}}
\index{Max\+Iterations@{Max\+Iterations}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Max\+Iterations()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ size\+\_\+t\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Max\+Iterations (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a99ee0230eedffd0dd9a20bc07024a4c8}


Modify the maximum number of iterations (0 indicates no limit). 



Definition at line 118 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::max\+Iterations.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Optimize@{Optimize}}
\index{Optimize@{Optimize}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Optimize(arma\+::mat \&iterate)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Optimize (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&}]{iterate}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1optimization_1_1AdaDelta_af8697def83fd2451446dcfe97b7c4088}


Optimize the given function using \doxyref{Ada\+Delta}{p.}{classmlpack_1_1optimization_1_1AdaDelta}. 

The given starting point will be modified to store the finishing point of the algorithm, and the final objective value is returned.


\begin{DoxyParams}{Parameters}
{\em iterate} & Starting point (will be modified). \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Objective value of the final point. 
\end{DoxyReturn}
\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Rho@{Rho}}
\index{Rho@{Rho}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Rho() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Rho (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a613a359459c13f08be2f7e1fa9a82ccc}


Get the smoothing parameter. 



Definition at line 106 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::rho.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Rho@{Rho}}
\index{Rho@{Rho}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Rho()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Rho (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a2ab5deb5aa954f0b7300fffdec0d46b9}


Modify the smoothing parameter. 



Definition at line 108 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::rho.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Shuffle@{Shuffle}}
\index{Shuffle@{Shuffle}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Shuffle() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Shuffle (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a05a6a896c254b958e12f33ca65d675f8}


Get whether or not the individual functions are shuffled. 



Definition at line 126 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::shuffle.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Shuffle@{Shuffle}}
\index{Shuffle@{Shuffle}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Shuffle()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Shuffle (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a8dc1f0b1a9f4e92065b3e807a04421b5}


Modify whether or not the individual functions are shuffled. 



Definition at line 128 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::shuffle.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Tolerance@{Tolerance}}
\index{Tolerance@{Tolerance}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Tolerance() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Tolerance (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_af982aa4fdc5b11f2d1f54a7aa3cf0b75}


Get the tolerance for termination. 



Definition at line 121 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::tolerance.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!Tolerance@{Tolerance}}
\index{Tolerance@{Tolerance}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{Tolerance()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::Tolerance (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a5608018b50e044d47c02ae96b5e817a1}


Modify the tolerance for termination. 



Definition at line 123 of file ada\+\_\+delta.\+hpp.



References mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::tolerance.



\subsection{Member Data Documentation}
\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!eps@{eps}}
\index{eps@{eps}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{eps}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::eps\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a8daa7c2a8b22f982cf1271c10334f046}


The value used to initialise the mean squared gradient parameter. 



Definition at line 138 of file ada\+\_\+delta.\+hpp.



Referenced by mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::\+Epsilon().

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!function@{function}}
\index{function@{function}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{function}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ Decomposable\+Function\+Type\& {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::function\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1AdaDelta_ab2803c3e4c5d6d2e67797c570d5a57cd}


The instantiated function. 



Definition at line 132 of file ada\+\_\+delta.\+hpp.

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!max\+Iterations@{max\+Iterations}}
\index{max\+Iterations@{max\+Iterations}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{max\+Iterations}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ size\+\_\+t {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::max\+Iterations\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a57d20d8ef11d20b0b064dfc0651f62c4}


The maximum number of allowed iterations. 



Definition at line 141 of file ada\+\_\+delta.\+hpp.



Referenced by mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::\+Max\+Iterations().

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!rho@{rho}}
\index{rho@{rho}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{rho}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::rho\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1AdaDelta_abff6a9dd8b5252baa7d7fcda79fa8cee}


The smoothing parameter. 



Definition at line 135 of file ada\+\_\+delta.\+hpp.



Referenced by mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::\+Rho().

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!shuffle@{shuffle}}
\index{shuffle@{shuffle}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{shuffle}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ bool {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::shuffle\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a3056efd630103c20bf94cd775974ca1b}


Controls whether or not the individual functions are shuffled when iterating. 



Definition at line 148 of file ada\+\_\+delta.\+hpp.



Referenced by mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::\+Shuffle().

\index{mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}!tolerance@{tolerance}}
\index{tolerance@{tolerance}!mlpack\+::optimization\+::\+Ada\+Delta@{mlpack\+::optimization\+::\+Ada\+Delta}}
\subsubsection[{tolerance}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ double {\bf mlpack\+::optimization\+::\+Ada\+Delta}$<$ Decomposable\+Function\+Type $>$\+::tolerance\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1optimization_1_1AdaDelta_a8cf0f7421523cd4592fc881f3f80d112}


The tolerance for termination. 



Definition at line 144 of file ada\+\_\+delta.\+hpp.



Referenced by mlpack\+::optimization\+::\+Ada\+Delta$<$ Decomposable\+Function\+Type $>$\+::\+Tolerance().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/mlpack/core/optimizers/adadelta/{\bf ada\+\_\+delta.\+hpp}\end{DoxyCompactItemize}
