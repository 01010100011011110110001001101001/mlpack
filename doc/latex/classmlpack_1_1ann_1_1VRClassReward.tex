\section{mlpack\+:\+:ann\+:\+:V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1VRClassReward}\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$@{mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$}}


Implementation of the variance reduced classification reinforcement layer.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\bf V\+R\+Class\+Reward} (const double {\bf scale}=1, const bool {\bf size\+Average}=true)
\begin{DoxyCompactList}\small\item\em Create the \doxyref{V\+R\+Class\+Reward}{p.}{classmlpack_1_1ann_1_1VRClassReward} object. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Layer\+Type , class... Args$>$ }\\void {\bf Add} (Args...\+args)
\item 
void {\bf Add} ({\bf Layer\+Types} layer)
\item 
{\footnotesize template$<$typename eT $>$ }\\void {\bf Backward} (const arma\+::\+Mat$<$ eT $>$ \&\&input, const arma\+::\+Mat$<$ eT $>$ \&\&target, arma\+::\+Mat$<$ eT $>$ \&\&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Delta} () const 
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
bool {\bf Deterministic} () const 
\begin{DoxyCompactList}\small\item\em Get the value of the deterministic parameter. \end{DoxyCompactList}\item 
bool \& {\bf Deterministic} ()
\begin{DoxyCompactList}\small\item\em Modify the value of the deterministic parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\double {\bf Forward} (const arma\+::\+Mat$<$ eT $>$ \&\&input, const arma\+::\+Mat$<$ eT $>$ \&\&target)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. \end{DoxyCompactList}\item 
Input\+Data\+Type \& {\bf Input\+Parameter} () const 
\begin{DoxyCompactList}\small\item\em Get the input parameter. \end{DoxyCompactList}\item 
Input\+Data\+Type \& {\bf Input\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the input parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Output\+Parameter} () const 
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void {\bf Serialize} (Archive \&, const unsigned int)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Attributes}
\begin{DoxyCompactItemize}
\item 
Output\+Data\+Type {\bf delta}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delta object. \end{DoxyCompactList}\item 
bool {\bf deterministic}
\begin{DoxyCompactList}\small\item\em If true dropout and scaling is disabled, see notes above. \end{DoxyCompactList}\item 
Input\+Data\+Type {\bf input\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored input parameter object. \end{DoxyCompactList}\item 
std\+::vector$<$ {\bf Layer\+Types} $>$ {\bf network}
\begin{DoxyCompactList}\small\item\em Locally-\/stored network modules. \end{DoxyCompactList}\item 
Output\+Data\+Type {\bf output\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output parameter object. \end{DoxyCompactList}\item 
double {\bf reward}
\begin{DoxyCompactList}\small\item\em Locally stored reward parameter. \end{DoxyCompactList}\item 
const double {\bf scale}
\begin{DoxyCompactList}\small\item\em Locally-\/stored value to scale the reward. \end{DoxyCompactList}\item 
const bool {\bf size\+Average}
\begin{DoxyCompactList}\small\item\em If true take the average over all batches. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\\*
class mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

Implementation of the variance reduced classification reinforcement layer. 

This layer is meant to be used in combination with the reinforce normal layer (Reinforce\+Normal\+Layer), which expects that an reward\+: (1 for success, 0 otherwise).


\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 54 of file layer\+\_\+types.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!V\+R\+Class\+Reward@{V\+R\+Class\+Reward}}
\index{V\+R\+Class\+Reward@{V\+R\+Class\+Reward}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{V\+R\+Class\+Reward(const double scale=1, const bool size\+Average=true)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::{\bf V\+R\+Class\+Reward} (
\begin{DoxyParamCaption}
\item[{const double}]{scale = {\ttfamily 1}, }
\item[{const bool}]{size\+Average = {\ttfamily true}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1VRClassReward_a3d7dba69f6c835d3e3b2ce122b6252ad}


Create the \doxyref{V\+R\+Class\+Reward}{p.}{classmlpack_1_1ann_1_1VRClassReward} object. 


\begin{DoxyParams}{Parameters}
{\em scale} & Parameter used to scale the reward. \\
\hline
{\em size\+Average} & Take the average over all batches. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Add(\+Args...\+args)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$class Layer\+Type , class... Args$>$ void {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{Args...}]{args}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_a004c931c050a9d2c390ebac32a05a576}


Definition at line 97 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Add(\+Layer\+Types layer)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ void {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{{\bf Layer\+Types}}]{layer}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_aa85a54b110f3b3271db1ab81ae57142a}


Definition at line 104 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::network, and mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Serialize().

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Backward(const arma\+::\+Mat$<$ e\+T $>$ \&\&input, const arma\+::\+Mat$<$ e\+T $>$ \&\&target, arma\+::\+Mat$<$ e\+T $>$ \&\&output)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename eT $>$ void {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Backward (
\begin{DoxyParamCaption}
\item[{const arma\+::\+Mat$<$ eT $>$ \&\&}]{input, }
\item[{const arma\+::\+Mat$<$ eT $>$ \&\&}]{target, }
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{output}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1VRClassReward_adf59606be43de5aebe1bbb7b39ce0d5f}


Ordinary feed backward pass of a neural network. 

The negative log likelihood layer expectes that the input contains log-\/probabilities for each class. The layer also expects a class index, in the range between 1 and the number of classes, as target when calling the Forward function.


\begin{DoxyParams}{Parameters}
{\em input} & The propagated input activation. \\
\hline
{\em target} & The target vector, that contains the class index in the range between 1 and the number of classes. \\
\hline
{\em output} & The calculated error. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Delta() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Delta (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_af73c46259522b232be83bded26cf39bb}


Get the delta. 



Definition at line 82 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Delta()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Delta (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_a05c607057881b2f7cb70751a41067713}


Modify the delta. 



Definition at line 84 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Deterministic() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ bool {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Deterministic (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_ab6ea5b289deb6d0afc87d263978241ab}


Get the value of the deterministic parameter. 



Definition at line 87 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::deterministic.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Deterministic()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ bool\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Deterministic (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_ac246fdd7aa686c504da026d87a17490c}


Modify the value of the deterministic parameter. 



Definition at line 89 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::deterministic.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Forward(const arma\+::\+Mat$<$ e\+T $>$ \&\&input, const arma\+::\+Mat$<$ e\+T $>$ \&\&target)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename eT $>$ double {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Forward (
\begin{DoxyParamCaption}
\item[{const arma\+::\+Mat$<$ eT $>$ \&\&}]{input, }
\item[{const arma\+::\+Mat$<$ eT $>$ \&\&}]{target}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1VRClassReward_ae344608a478f662265f50da419740992}


Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data that contains the log-\/probabilities for each class. \\
\hline
{\em target} & The target vector, that contains the class index in the range between 1 and the number of classes. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Input\+Parameter@{Input\+Parameter}}
\index{Input\+Parameter@{Input\+Parameter}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Input\+Parameter() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Input\+Data\+Type\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Input\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_ab044f309bd422ac79c525b64410a7782}


Get the input parameter. 



Definition at line 72 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::input\+Parameter.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Input\+Parameter@{Input\+Parameter}}
\index{Input\+Parameter@{Input\+Parameter}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Input\+Parameter()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Input\+Data\+Type\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Input\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_a19887cbe20394cbdb24d201221b1a2f9}


Modify the input parameter. 



Definition at line 74 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::input\+Parameter.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Output\+Parameter() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Output\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_a47a180ba92e2c557c96cdbf5329e27e9}


Get the output parameter. 



Definition at line 77 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Output\+Parameter()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Output\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1VRClassReward_ab7297a01b8e76ee3a710d922eb4d9ce0}


Modify the output parameter. 



Definition at line 79 of file vr\+\_\+class\+\_\+reward.\+hpp.



References mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!Serialize@{Serialize}}
\index{Serialize@{Serialize}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{Serialize(\+Archive \&, const unsigned int)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename Archive $>$ void {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Serialize (
\begin{DoxyParamCaption}
\item[{Archive \&}]{, }
\item[{const unsigned}]{int}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1VRClassReward_a6f259ea0b57843f1d90814bb6713b5c9}


Serialize the layer. 



Referenced by mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Add().



\subsection{Member Data Documentation}
\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!delta@{delta}}
\index{delta@{delta}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{delta}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_aae6acd40a3dc5d8f1b48fbaffc74efc9}


Locally-\/stored delta object. 



Definition at line 123 of file vr\+\_\+class\+\_\+reward.\+hpp.



Referenced by mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Delta().

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!deterministic@{deterministic}}
\index{deterministic@{deterministic}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{deterministic}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ bool {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::deterministic\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_a108511cc22fe0333baccff15792b399c}


If true dropout and scaling is disabled, see notes above. 



Definition at line 132 of file vr\+\_\+class\+\_\+reward.\+hpp.



Referenced by mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Deterministic().

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!input\+Parameter@{input\+Parameter}}
\index{input\+Parameter@{input\+Parameter}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{input\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Input\+Data\+Type {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::input\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_a6c5c6145dc82347cda0ecd9e75c42b32}


Locally-\/stored input parameter object. 



Definition at line 126 of file vr\+\_\+class\+\_\+reward.\+hpp.



Referenced by mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Input\+Parameter().

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!network@{network}}
\index{network@{network}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{network}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ std\+::vector$<${\bf Layer\+Types}$>$ {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::network\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_aecfc04db8b5fc69958b19850dee70f4b}


Locally-\/stored network modules. 



Definition at line 135 of file vr\+\_\+class\+\_\+reward.\+hpp.



Referenced by mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Add().

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!output\+Parameter@{output\+Parameter}}
\index{output\+Parameter@{output\+Parameter}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{output\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_adae696b996c873bf1125cf641b68790a}


Locally-\/stored output parameter object. 



Definition at line 129 of file vr\+\_\+class\+\_\+reward.\+hpp.



Referenced by mlpack\+::ann\+::\+V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Output\+Parameter().

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!reward@{reward}}
\index{reward@{reward}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{reward}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ double {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::reward\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_ae9d81f4a6fce7f06b92160ce9d04fe30}


Locally stored reward parameter. 



Definition at line 120 of file vr\+\_\+class\+\_\+reward.\+hpp.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!scale@{scale}}
\index{scale@{scale}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{scale}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ const double {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::scale\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_a1fe23aa92c415f614c0f8a68b8525c05}


Locally-\/stored value to scale the reward. 



Definition at line 114 of file vr\+\_\+class\+\_\+reward.\+hpp.

\index{mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}!size\+Average@{size\+Average}}
\index{size\+Average@{size\+Average}!mlpack\+::ann\+::\+V\+R\+Class\+Reward@{mlpack\+::ann\+::\+V\+R\+Class\+Reward}}
\subsubsection[{size\+Average}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ const bool {\bf mlpack\+::ann\+::\+V\+R\+Class\+Reward}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::size\+Average\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1VRClassReward_af7740d45c865b320b3cfc38c143f9eb3}


If true take the average over all batches. 



Definition at line 117 of file vr\+\_\+class\+\_\+reward.\+hpp.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
src/mlpack/methods/ann/layer/{\bf layer\+\_\+types.\+hpp}\item 
src/mlpack/methods/ann/layer/{\bf vr\+\_\+class\+\_\+reward.\+hpp}\end{DoxyCompactItemize}
