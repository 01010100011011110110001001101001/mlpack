\section{mlpack\+:\+:optimization Namespace Reference}
\label{namespacemlpack_1_1optimization}\index{mlpack\+::optimization@{mlpack\+::optimization}}
\subsection*{Namespaces}
\begin{DoxyCompactItemize}
\item 
 {\bf test}
\end{DoxyCompactItemize}
\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class {\bf Ada\+Delta}
\begin{DoxyCompactList}\small\item\em Adadelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad method\+: \end{DoxyCompactList}\item 
class {\bf Adam}
\begin{DoxyCompactList}\small\item\em \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. \end{DoxyCompactList}\item 
class {\bf Aug\+Lagrangian}
\begin{DoxyCompactList}\small\item\em The \doxyref{Aug\+Lagrangian}{p.}{classmlpack_1_1optimization_1_1AugLagrangian} class implements the Augmented Lagrangian method of optimization. \end{DoxyCompactList}\item 
class {\bf Aug\+Lagrangian\+Function}
\begin{DoxyCompactList}\small\item\em This is a utility class used by \doxyref{Aug\+Lagrangian}{p.}{classmlpack_1_1optimization_1_1AugLagrangian}, meant to wrap a Lagrangian\+Function into a function usable by a simple optimizer like L-\/\+B\+F\+GS. \end{DoxyCompactList}\item 
class {\bf Aug\+Lagrangian\+Test\+Function}
\begin{DoxyCompactList}\small\item\em This function is taken from \char`\"{}\+Practical Mathematical Optimization\char`\"{} (Snyman), section 5.\+3.\+8 (\char`\"{}\+Application of the Augmented Lagrangian Method\char`\"{}). \end{DoxyCompactList}\item 
class {\bf Exponential\+Schedule}
\begin{DoxyCompactList}\small\item\em The exponential cooling schedule cools the temperature T at every step according to the equation. \end{DoxyCompactList}\item 
class {\bf Gockenbach\+Function}
\begin{DoxyCompactList}\small\item\em This function is taken from M. \end{DoxyCompactList}\item 
class {\bf Gradient\+Descent}
\begin{DoxyCompactList}\small\item\em Gradient Descent is a technique to minimize a function. \end{DoxyCompactList}\item 
class {\bf L\+\_\+\+B\+F\+GS}
\begin{DoxyCompactList}\small\item\em The generic L-\/\+B\+F\+GS optimizer, which uses a back-\/tracking line search algorithm to minimize a function. \end{DoxyCompactList}\item 
class {\bf Lovasz\+Theta\+S\+DP}
\begin{DoxyCompactList}\small\item\em This function is the Lovasz-\/\+Theta semidefinite program, as implemented in the following paper\+: \end{DoxyCompactList}\item 
class {\bf L\+R\+S\+DP}
\begin{DoxyCompactList}\small\item\em \doxyref{L\+R\+S\+DP}{p.}{classmlpack_1_1optimization_1_1LRSDP} is the implementation of Monteiro and Burer\textquotesingle{}s formulation of low-\/rank semidefinite programs (L\+R-\/\+S\+DP). \end{DoxyCompactList}\item 
class {\bf L\+R\+S\+D\+P\+Function}
\begin{DoxyCompactList}\small\item\em The objective function that \doxyref{L\+R\+S\+DP}{p.}{classmlpack_1_1optimization_1_1LRSDP} is trying to optimize. \end{DoxyCompactList}\item 
class {\bf Mini\+Batch\+S\+GD}
\begin{DoxyCompactList}\small\item\em Mini-\/batch Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions. \end{DoxyCompactList}\item 
class {\bf Primal\+Dual\+Solver}
\begin{DoxyCompactList}\small\item\em Interface to a primal dual interior point solver. \end{DoxyCompactList}\item 
class {\bf R\+M\+Sprop}
\begin{DoxyCompactList}\small\item\em \doxyref{R\+M\+Sprop}{p.}{classmlpack_1_1optimization_1_1RMSprop} is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients. \end{DoxyCompactList}\item 
class {\bf SA}
\begin{DoxyCompactList}\small\item\em Simulated Annealing is an stochastic optimization algorithm which is able to deliver near-\/optimal results quickly without knowing the gradient of the function being optimized. \end{DoxyCompactList}\item 
class {\bf S\+DP}
\begin{DoxyCompactList}\small\item\em Specify an \doxyref{S\+DP}{p.}{classmlpack_1_1optimization_1_1SDP} in primal form. \end{DoxyCompactList}\item 
class {\bf S\+GD}
\begin{DoxyCompactList}\small\item\em Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions. \end{DoxyCompactList}\item 
class {\bf Vanilla\+Update}
\begin{DoxyCompactList}\small\item\em Vanilla update policy for Stochastic Gradient Descent (\doxyref{S\+GD}{p.}{classmlpack_1_1optimization_1_1SGD}). \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Typedefs}
\begin{DoxyCompactItemize}
\item 
{\footnotesize template$<$typename Decomposable\+Function\+Type $>$ }\\using {\bf Momentum\+S\+GD} = {\bf S\+GD}$<$ Decomposable\+Function\+Type, Momentum\+Update $>$
\item 
{\footnotesize template$<$typename Decomposable\+Function\+Type $>$ }\\using {\bf Standard\+S\+GD} = {\bf S\+GD}$<$ Decomposable\+Function\+Type, {\bf Vanilla\+Update} $>$
\end{DoxyCompactItemize}


\subsection{Typedef Documentation}
\index{mlpack\+::optimization@{mlpack\+::optimization}!Momentum\+S\+GD@{Momentum\+S\+GD}}
\index{Momentum\+S\+GD@{Momentum\+S\+GD}!mlpack\+::optimization@{mlpack\+::optimization}}
\subsubsection[{Momentum\+S\+GD}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ using {\bf mlpack\+::optimization\+::\+Momentum\+S\+GD} = typedef {\bf S\+GD}$<$Decomposable\+Function\+Type, Momentum\+Update$>$}\label{namespacemlpack_1_1optimization_a54725aba08fe3d4e3c232b784122cf0b}


Definition at line 163 of file sgd.\+hpp.

\index{mlpack\+::optimization@{mlpack\+::optimization}!Standard\+S\+GD@{Standard\+S\+GD}}
\index{Standard\+S\+GD@{Standard\+S\+GD}!mlpack\+::optimization@{mlpack\+::optimization}}
\subsubsection[{Standard\+S\+GD}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Decomposable\+Function\+Type $>$ using {\bf mlpack\+::optimization\+::\+Standard\+S\+GD} = typedef {\bf S\+GD}$<$Decomposable\+Function\+Type, {\bf Vanilla\+Update}$>$}\label{namespacemlpack_1_1optimization_a3a601b2b23932fd95e02b8efc599a46f}


Definition at line 160 of file sgd.\+hpp.

