\section{mlpack\+:\+:ann\+:\+:R\+NN$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1RNN}\index{mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$@{mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$}}


Implementation of a standard recurrent neural network container.  


\subsection*{Public Types}
\begin{DoxyCompactItemize}
\item 
using {\bf Network\+Type} = {\bf R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$
\begin{DoxyCompactList}\small\item\em Convenience typedef for the internal model construction. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\bf R\+NN} (const size\+\_\+t {\bf rho}, const bool {\bf single}=false, Output\+Layer\+Type {\bf output\+Layer}=Output\+Layer\+Type(), Initialization\+Rule\+Type {\bf initialize\+Rule}=Initialization\+Rule\+Type())
\begin{DoxyCompactList}\small\item\em Create the \doxyref{R\+NN}{p.}{classmlpack_1_1ann_1_1RNN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. \end{DoxyCompactList}\item 
{\bf R\+NN} (const arma\+::mat \&{\bf predictors}, const arma\+::mat \&{\bf responses}, const size\+\_\+t {\bf rho}, const bool {\bf single}=false, Output\+Layer\+Type {\bf output\+Layer}=Output\+Layer\+Type(), Initialization\+Rule\+Type {\bf initialize\+Rule}=Initialization\+Rule\+Type())
\begin{DoxyCompactList}\small\item\em Create the \doxyref{R\+NN}{p.}{classmlpack_1_1ann_1_1RNN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. \end{DoxyCompactList}\item 
{\bf $\sim$\+R\+NN} ()
\begin{DoxyCompactList}\small\item\em Destructor to release allocated memory. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Layer\+Type $>$ }\\void {\bf Add} (const Layer\+Type \&layer)
\item 
{\footnotesize template$<$class Layer\+Type , class... Args$>$ }\\void {\bf Add} (Args...\+args)
\item 
void {\bf Add} ({\bf Layer\+Types} layer)
\item 
double {\bf Evaluate} (const arma\+::mat \&, const size\+\_\+t i, const bool {\bf deterministic}=true)
\begin{DoxyCompactList}\small\item\em Evaluate the recurrent neural network with the given parameters. \end{DoxyCompactList}\item 
void {\bf Gradient} (const arma\+::mat \&parameters, const size\+\_\+t i, arma\+::mat \&gradient)
\begin{DoxyCompactList}\small\item\em Evaluate the gradient of the recurrent neural network with the given parameters, and with respect to only one point in the dataset. \end{DoxyCompactList}\item 
size\+\_\+t {\bf Num\+Functions} () const 
\begin{DoxyCompactList}\small\item\em Return the number of separable functions (the number of predictor points). \end{DoxyCompactList}\item 
const arma\+::mat \& {\bf Parameters} () const 
\begin{DoxyCompactList}\small\item\em Return the initial point for the optimization. \end{DoxyCompactList}\item 
arma\+::mat \& {\bf Parameters} ()
\begin{DoxyCompactList}\small\item\em Modify the initial point for the optimization. \end{DoxyCompactList}\item 
void {\bf Predict} (arma\+::mat \&{\bf predictors}, arma\+::mat \&{\bf responses})
\begin{DoxyCompactList}\small\item\em Predict the responses to a given set of predictors. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void {\bf Serialize} (Archive \&ar, const unsigned int)
\begin{DoxyCompactList}\small\item\em Serialize the model. \end{DoxyCompactList}\item 
{\footnotesize template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+Standard\+S\+GD$>$ }\\void {\bf Train} (const arma\+::mat \&{\bf predictors}, const arma\+::mat \&{\bf responses}, Optimizer\+Type$<$ {\bf Network\+Type} $>$ \&optimizer)
\begin{DoxyCompactList}\small\item\em Train the recurrent neural network on the given input data using the given optimizer. \end{DoxyCompactList}\item 
{\footnotesize template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+Standard\+S\+GD$>$ }\\void {\bf Train} (const arma\+::mat \&{\bf predictors}, const arma\+::mat \&{\bf responses})
\begin{DoxyCompactList}\small\item\em Train the recurrent neural network on the given input data. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Member Functions}
\begin{DoxyCompactItemize}
\item 
void {\bf Backward} ()
\begin{DoxyCompactList}\small\item\em The Backward algorithm (part of the Forward-\/\+Backward algorithm). \end{DoxyCompactList}\item 
void {\bf Forward} (arma\+::mat \&\&input)
\begin{DoxyCompactList}\small\item\em The Forward algorithm (part of the Forward-\/\+Backward algorithm). \end{DoxyCompactList}\item 
void {\bf Gradient} ()
\begin{DoxyCompactList}\small\item\em Iterate through all layer modules and update the the gradient using the layer defined optimizer. \end{DoxyCompactList}\item 
void {\bf Reset\+Deterministic} ()
\begin{DoxyCompactList}\small\item\em Reset the module status by setting the current deterministic parameter for all modules that implement the Deterministic function. \end{DoxyCompactList}\item 
void {\bf Reset\+Gradients} (arma\+::mat \&gradient)
\begin{DoxyCompactList}\small\item\em Reset the gradient for all modules that implement the Gradient function. \end{DoxyCompactList}\item 
void {\bf Reset\+Parameters} ()
\begin{DoxyCompactList}\small\item\em Reset the module infomration (weights/parameters). \end{DoxyCompactList}\item 
void {\bf Single\+Predict} (const arma\+::mat \&{\bf predictors}, arma\+::mat \&{\bf responses})
\end{DoxyCompactItemize}
\subsection*{Private Attributes}
\begin{DoxyCompactItemize}
\item 
arma\+::mat {\bf current\+Input}
\begin{DoxyCompactList}\small\item\em T\+He current input of the forward/backward pass. \end{DoxyCompactList}\item 
{\bf Delete\+Visitor} {\bf delete\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delete visitor. \end{DoxyCompactList}\item 
{\bf Delta\+Visitor} {\bf delta\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delta visitor. \end{DoxyCompactList}\item 
bool {\bf deterministic}
\begin{DoxyCompactList}\small\item\em The current evaluation mode (training or testing). \end{DoxyCompactList}\item 
arma\+::mat {\bf error}
\begin{DoxyCompactList}\small\item\em The current error for the backward pass. \end{DoxyCompactList}\item 
Initialization\+Rule\+Type {\bf initialize\+Rule}
\begin{DoxyCompactList}\small\item\em Instantiated Initialization\+Rule object for initializing the network parameter. \end{DoxyCompactList}\item 
size\+\_\+t {\bf input\+Size}
\begin{DoxyCompactList}\small\item\em The input size. \end{DoxyCompactList}\item 
std\+::vector$<$ arma\+::mat $>$ {\bf module\+Output\+Parameter}
\begin{DoxyCompactList}\small\item\em List of all module parameters for the backward pass (B\+B\+TT). \end{DoxyCompactList}\item 
std\+::vector$<$ {\bf Layer\+Types} $>$ {\bf network}
\begin{DoxyCompactList}\small\item\em Locally-\/stored model modules. \end{DoxyCompactList}\item 
size\+\_\+t {\bf num\+Functions}
\begin{DoxyCompactList}\small\item\em The number of separable functions (the number of predictor points). \end{DoxyCompactList}\item 
Output\+Layer\+Type {\bf output\+Layer}
\begin{DoxyCompactList}\small\item\em Instantiated outputlayer used to evaluate the network. \end{DoxyCompactList}\item 
{\bf Output\+Parameter\+Visitor} {\bf output\+Parameter\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output parameter visitor. \end{DoxyCompactList}\item 
size\+\_\+t {\bf output\+Size}
\begin{DoxyCompactList}\small\item\em The output size. \end{DoxyCompactList}\item 
arma\+::mat {\bf parameter}
\begin{DoxyCompactList}\small\item\em Matrix of (trained) parameters. \end{DoxyCompactList}\item 
arma\+::mat {\bf predictors}
\begin{DoxyCompactList}\small\item\em The matrix of data points (predictors). \end{DoxyCompactList}\item 
bool {\bf reset}
\begin{DoxyCompactList}\small\item\em Indicator if we already trained the model. \end{DoxyCompactList}\item 
{\bf Reset\+Visitor} {\bf reset\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored reset visitor. \end{DoxyCompactList}\item 
arma\+::mat {\bf responses}
\begin{DoxyCompactList}\small\item\em The matrix of responses to the input data points. \end{DoxyCompactList}\item 
size\+\_\+t {\bf rho}
\begin{DoxyCompactList}\small\item\em Number of steps to backpropagate through time (B\+P\+TT). \end{DoxyCompactList}\item 
bool {\bf single}
\begin{DoxyCompactList}\small\item\em Only predict the last element of the input sequence. \end{DoxyCompactList}\item 
size\+\_\+t {\bf target\+Size}
\begin{DoxyCompactList}\small\item\em The target size. \end{DoxyCompactList}\item 
{\bf Weight\+Size\+Visitor} {\bf weight\+Size\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored weight size visitor. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Output\+Layer\+Type = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type = Random\+Initialization$>$\\*
class mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$}

Implementation of a standard recurrent neural network container. 


\begin{DoxyTemplParams}{Template Parameters}
{\em Output\+Layer\+Type} & The output layer type used to evaluate the network. \\
\hline
{\em Initialization\+Rule\+Type} & Rule used to initialize the weight matrix. \\
\hline
\end{DoxyTemplParams}


Definition at line 40 of file rnn.\+hpp.



\subsection{Member Typedef Documentation}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Network\+Type@{Network\+Type}}
\index{Network\+Type@{Network\+Type}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Network\+Type}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ using {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Network\+Type} =  {\bf R\+NN}$<$Output\+Layer\+Type, Initialization\+Rule\+Type$>$}\label{classmlpack_1_1ann_1_1RNN_a7fe3f40e16983e49567690459958e63e}


Convenience typedef for the internal model construction. 



Definition at line 44 of file rnn.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!R\+NN@{R\+NN}}
\index{R\+NN@{R\+NN}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{R\+N\+N(const size\+\_\+t rho, const bool single=false, Output\+Layer\+Type output\+Layer=\+Output\+Layer\+Type(), Initialization\+Rule\+Type initialize\+Rule=\+Initialization\+Rule\+Type())}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf R\+NN} (
\begin{DoxyParamCaption}
\item[{const size\+\_\+t}]{rho, }
\item[{const bool}]{single = {\ttfamily false}, }
\item[{Output\+Layer\+Type}]{output\+Layer = {\ttfamily OutputLayerType()}, }
\item[{Initialization\+Rule\+Type}]{initialize\+Rule = {\ttfamily InitializationRuleType()}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_a8b2407c392dddfe13e890790d5ca9b17}


Create the \doxyref{R\+NN}{p.}{classmlpack_1_1ann_1_1RNN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. 

Optionally, specify which initialize rule and performance function should be used.


\begin{DoxyParams}{Parameters}
{\em rho} & Maximum number of steps to backpropagate through time (B\+P\+TT). \\
\hline
{\em single} & Predict only the last element of the input sequence. \\
\hline
{\em output\+Layer} & Output layer used to evaluate the network. \\
\hline
{\em initialize\+Rule} & Optional instantiated Initialization\+Rule object for initializing the network parameter. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!R\+NN@{R\+NN}}
\index{R\+NN@{R\+NN}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{R\+N\+N(const arma\+::mat \&predictors, const arma\+::mat \&responses, const size\+\_\+t rho, const bool single=false, Output\+Layer\+Type output\+Layer=\+Output\+Layer\+Type(), Initialization\+Rule\+Type initialize\+Rule=\+Initialization\+Rule\+Type())}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf R\+NN} (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{const arma\+::mat \&}]{responses, }
\item[{const size\+\_\+t}]{rho, }
\item[{const bool}]{single = {\ttfamily false}, }
\item[{Output\+Layer\+Type}]{output\+Layer = {\ttfamily OutputLayerType()}, }
\item[{Initialization\+Rule\+Type}]{initialize\+Rule = {\ttfamily InitializationRuleType()}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_a07d6042c10580b9b165e8225e9ee6cee}


Create the \doxyref{R\+NN}{p.}{classmlpack_1_1ann_1_1RNN} object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer. 

Optionally, specify which initialize rule and performance function should be used.


\begin{DoxyParams}{Parameters}
{\em predictors} & Input training variables. \\
\hline
{\em responses} & Outputs results from input training variables. \\
\hline
{\em rho} & Maximum number of steps to backpropagate through time (B\+P\+TT). \\
\hline
{\em single} & Predict only the last element of the input sequence. \\
\hline
{\em output\+Layer} & Output layer used to evaluate the network. \\
\hline
{\em initialize\+Rule} & Optional instantiated Initialization\+Rule object for initializing the network parameter. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!````~R\+NN@{$\sim$\+R\+NN}}
\index{````~R\+NN@{$\sim$\+R\+NN}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{$\sim$\+R\+N\+N()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::$\sim${\bf R\+NN} (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_af514c16592d13b87bec119f671e0b45b}


Destructor to release allocated memory. 



\subsection{Member Function Documentation}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Add(const Layer\+Type \&layer)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$typename Layer\+Type $>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{const Layer\+Type \&}]{layer}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RNN_a48d81c7b42355f9fe259bfe0a71525ed}


Definition at line 168 of file rnn.\+hpp.



References mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Add(\+Args...\+args)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$class Layer\+Type , class... Args$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{Args...}]{args}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RNN_a4ca8786c54768027d3d957ac475a977a}


Definition at line 176 of file rnn.\+hpp.



References mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Add@{Add}}
\index{Add@{Add}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Add(\+Layer\+Types layer)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::{\bf Add} (
\begin{DoxyParamCaption}
\item[{{\bf Layer\+Types}}]{layer}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RNN_aecb5a6f994b1770f46465d6b75290a83}


Definition at line 183 of file rnn.\+hpp.



References mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Backward()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Backward (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a8d5cd6ac122a10f265b96c769e339b7b}


The Backward algorithm (part of the Forward-\/\+Backward algorithm). 

Computes backward pass for module. 

Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Evaluate@{Evaluate}}
\index{Evaluate@{Evaluate}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Evaluate(const arma\+::mat \&, const size\+\_\+t i, const bool deterministic=true)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ double {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Evaluate (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{, }
\item[{const size\+\_\+t}]{i, }
\item[{const bool}]{deterministic = {\ttfamily true}}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_a4998a414624de0d2985280d7abfaf3d9}


Evaluate the recurrent neural network with the given parameters. 

This function is usually called by the optimizer to train the model.


\begin{DoxyParams}{Parameters}
{\em parameters} & Matrix model parameters. \\
\hline
{\em i} & Index of point to use for objective function evaluation. \\
\hline
{\em deterministic} & Whether or not to train or test the model. Note some layer act differently in training or testing mode. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Forward(arma\+::mat \&\&input)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Forward (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&\&}]{input}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_ab908da54d633299a42b749b2ac3d4552}


The Forward algorithm (part of the Forward-\/\+Backward algorithm). 

Computes forward probabilities for each module.


\begin{DoxyParams}{Parameters}
{\em input} & Data sequence to compute probabilities for. \\
\hline
\end{DoxyParams}


Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Gradient(const arma\+::mat \&parameters, const size\+\_\+t i, arma\+::mat \&gradient)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{parameters, }
\item[{const size\+\_\+t}]{i, }
\item[{arma\+::mat \&}]{gradient}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_af243a73555de08b6006c681155c04de6}


Evaluate the gradient of the recurrent neural network with the given parameters, and with respect to only one point in the dataset. 

This is useful for optimizers such as S\+GD, which require a separable objective function.


\begin{DoxyParams}{Parameters}
{\em parameters} & Matrix of the model parameters to be optimized. \\
\hline
{\em i} & Index of points to use for objective function gradient evaluation. \\
\hline
{\em gradient} & Matrix to output gradient into. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Gradient()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a5bc5a562cce79dd1fa377f0d6e1ac090}


Iterate through all layer modules and update the the gradient using the layer defined optimizer. 



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Num\+Functions@{Num\+Functions}}
\index{Num\+Functions@{Num\+Functions}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Num\+Functions() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Num\+Functions (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RNN_ace974e0ffc8ce9afbf1cb66c30a77616}


Return the number of separable functions (the number of predictor points). 



Definition at line 186 of file rnn.\+hpp.



References mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::num\+Functions.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Parameters() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ const arma\+::mat\& {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RNN_a481790a45dd3cf1441d08805de6e4e10}


Return the initial point for the optimization. 



Definition at line 189 of file rnn.\+hpp.



References mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::parameter.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Parameters()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat\& {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RNN_aeaad2ebf3c00999459cd30351ea52d9c}


Modify the initial point for the optimization. 



Definition at line 191 of file rnn.\+hpp.



References mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Backward(), mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Forward(), mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Gradient(), mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::parameter, mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Reset\+Deterministic(), mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Reset\+Gradients(), mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Reset\+Parameters(), mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Serialize(), and mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Single\+Predict().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Predict@{Predict}}
\index{Predict@{Predict}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Predict(arma\+::mat \&predictors, arma\+::mat \&responses)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Predict (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&}]{predictors, }
\item[{arma\+::mat \&}]{responses}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_adb34b68774a0dc10af834584ab9d3787}


Predict the responses to a given set of predictors. 

The responses will reflect the output of the given output layer as returned by the output layer function.


\begin{DoxyParams}{Parameters}
{\em predictors} & Input predictors. \\
\hline
{\em responses} & Matrix to put output predictions of responses into. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Reset\+Deterministic@{Reset\+Deterministic}}
\index{Reset\+Deterministic@{Reset\+Deterministic}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Reset\+Deterministic()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Reset\+Deterministic (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a434a0fec69daad59ccfdf1dab95a71d4}


Reset the module status by setting the current deterministic parameter for all modules that implement the Deterministic function. 



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Reset\+Gradients@{Reset\+Gradients}}
\index{Reset\+Gradients@{Reset\+Gradients}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Reset\+Gradients(arma\+::mat \&gradient)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Reset\+Gradients (
\begin{DoxyParamCaption}
\item[{arma\+::mat \&}]{gradient}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_aea2e221c136342d54c9bebdd9599219e}


Reset the gradient for all modules that implement the Gradient function. 



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Reset\+Parameters@{Reset\+Parameters}}
\index{Reset\+Parameters@{Reset\+Parameters}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Reset\+Parameters()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Reset\+Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a215b8ed02c7c644aad17d57f7f6beb7c}


Reset the module infomration (weights/parameters). 



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Serialize@{Serialize}}
\index{Serialize@{Serialize}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Serialize(\+Archive \&ar, const unsigned int)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$typename Archive $>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Serialize (
\begin{DoxyParamCaption}
\item[{Archive \&}]{ar, }
\item[{const unsigned}]{int}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_abdfa2d68bb7c68c47a5223f0097eeb96}


Serialize the model. 



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Single\+Predict@{Single\+Predict}}
\index{Single\+Predict@{Single\+Predict}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Single\+Predict(const arma\+::mat \&predictors, arma\+::mat \&responses)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Single\+Predict (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{arma\+::mat \&}]{responses}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_ae3c44882be75a13e4f53eee4131aba23}


Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Train@{Train}}
\index{Train@{Train}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Train(const arma\+::mat \&predictors, const arma\+::mat \&responses, Optimizer\+Type$<$ Network\+Type $>$ \&optimizer)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+Standard\+S\+GD$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Train (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{const arma\+::mat \&}]{responses, }
\item[{Optimizer\+Type$<$ {\bf Network\+Type} $>$ \&}]{optimizer}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_aa144efe7e1f98c2be93f39a79c4982f5}


Train the recurrent neural network on the given input data using the given optimizer. 

This will use the existing model parameters as a starting point for the optimization. If this is not what you want, then you should access the parameters vector directly with \doxyref{Parameters()}{p.}{classmlpack_1_1ann_1_1RNN_aeaad2ebf3c00999459cd30351ea52d9c} and modify it as desired.


\begin{DoxyTemplParams}{Template Parameters}
{\em Optimizer\+Type} & Type of optimizer to use to train the model. \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em predictors} & Input training variables. \\
\hline
{\em responses} & Outputs results from input training variables. \\
\hline
{\em optimizer} & Instantiated optimizer used to train the model. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!Train@{Train}}
\index{Train@{Train}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{Train(const arma\+::mat \&predictors, const arma\+::mat \&responses)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ template$<$template$<$ typename $>$ class Optimizer\+Type = mlpack\+::optimization\+::\+Standard\+S\+GD$>$ void {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::Train (
\begin{DoxyParamCaption}
\item[{const arma\+::mat \&}]{predictors, }
\item[{const arma\+::mat \&}]{responses}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RNN_ad2903483064bb2c1b9daa232f949ace3}


Train the recurrent neural network on the given input data. 

By default, the S\+GD optimization algorithm is used, but others can be specified (such as \doxyref{mlpack\+::optimization\+::\+R\+M\+Sprop}{p.}{classmlpack_1_1optimization_1_1RMSprop}).

This will use the existing model parameters as a starting point for the optimization. If this is not what you want, then you should access the parameters vector directly with \doxyref{Parameters()}{p.}{classmlpack_1_1ann_1_1RNN_aeaad2ebf3c00999459cd30351ea52d9c} and modify it as desired.


\begin{DoxyTemplParams}{Template Parameters}
{\em Optimizer\+Type} & Type of optimizer to use to train the model. \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em predictors} & Input training variables. \\
\hline
{\em responses} & Outputs results from input training variables. \\
\hline
\end{DoxyParams}


\subsection{Member Data Documentation}
\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!current\+Input@{current\+Input}}
\index{current\+Input@{current\+Input}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{current\+Input}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::current\+Input\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_adc9be34740bd9da29e7e648667f91094}


T\+He current input of the forward/backward pass. 



Definition at line 287 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!delete\+Visitor@{delete\+Visitor}}
\index{delete\+Visitor@{delete\+Visitor}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{delete\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Delete\+Visitor} {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::delete\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a1a1a8db8d4f76cb0995a49f524325060}


Locally-\/stored delete visitor. 



Definition at line 305 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!delta\+Visitor@{delta\+Visitor}}
\index{delta\+Visitor@{delta\+Visitor}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{delta\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Delta\+Visitor} {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::delta\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a57a1993f3c0cefee06d106b9bd68025c}


Locally-\/stored delta visitor. 



Definition at line 290 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!deterministic@{deterministic}}
\index{deterministic@{deterministic}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{deterministic}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ bool {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::deterministic\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a22697aa22257c189403910c69d9e4386}


The current evaluation mode (training or testing). 



Definition at line 308 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!error@{error}}
\index{error@{error}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{error}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::error\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_acb5cefe5af510735a51c02d946fcfb58}


The current error for the backward pass. 



Definition at line 284 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!initialize\+Rule@{initialize\+Rule}}
\index{initialize\+Rule@{initialize\+Rule}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{initialize\+Rule}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ Initialization\+Rule\+Type {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::initialize\+Rule\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a64a31f246a5f15b639eab5182cb0c9b3}


Instantiated Initialization\+Rule object for initializing the network parameter. 



Definition at line 251 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!input\+Size@{input\+Size}}
\index{input\+Size@{input\+Size}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{input\+Size}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::input\+Size\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a0b4965081bcc535dfea271f8918e8ff2}


The input size. 



Definition at line 254 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!module\+Output\+Parameter@{module\+Output\+Parameter}}
\index{module\+Output\+Parameter@{module\+Output\+Parameter}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{module\+Output\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ std\+::vector$<$arma\+::mat$>$ {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::module\+Output\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a9e90e2c565468e9dcf33e305e04297fd}


List of all module parameters for the backward pass (B\+B\+TT). 



Definition at line 296 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!network@{network}}
\index{network@{network}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{network}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ std\+::vector$<${\bf Layer\+Types}$>$ {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::network\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a4049c23e47dfd069810fea73d6e93aa8}


Locally-\/stored model modules. 



Definition at line 269 of file rnn.\+hpp.



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Add().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!num\+Functions@{num\+Functions}}
\index{num\+Functions@{num\+Functions}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{num\+Functions}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::num\+Functions\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a4597b72b95e107847828d4283add180c}


The number of separable functions (the number of predictor points). 



Definition at line 281 of file rnn.\+hpp.



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Num\+Functions().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!output\+Layer@{output\+Layer}}
\index{output\+Layer@{output\+Layer}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{output\+Layer}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ Output\+Layer\+Type {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Layer\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a4c86c4c8a443daeb1d19669260359f66}


Instantiated outputlayer used to evaluate the network. 



Definition at line 247 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!output\+Parameter\+Visitor@{output\+Parameter\+Visitor}}
\index{output\+Parameter\+Visitor@{output\+Parameter\+Visitor}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{output\+Parameter\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Output\+Parameter\+Visitor} {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Parameter\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a0a3e30be54387448a4a678ee04e89472}


Locally-\/stored output parameter visitor. 



Definition at line 293 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!output\+Size@{output\+Size}}
\index{output\+Size@{output\+Size}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{output\+Size}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::output\+Size\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_ad5e15ca242906bbaa497490ae1f6e3b2}


The output size. 



Definition at line 257 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!parameter@{parameter}}
\index{parameter@{parameter}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_acb2363276833b041e9cc04efe3e6da7a}


Matrix of (trained) parameters. 



Definition at line 278 of file rnn.\+hpp.



Referenced by mlpack\+::ann\+::\+R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!predictors@{predictors}}
\index{predictors@{predictors}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{predictors}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::predictors\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_abb896f990996968bb7fed3d284e2b3e1}


The matrix of data points (predictors). 



Definition at line 272 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!reset@{reset}}
\index{reset@{reset}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{reset}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ bool {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::reset\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a8eaaf6b15ce560a7e31d00d94108a8d0}


Indicator if we already trained the model. 



Definition at line 263 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!reset\+Visitor@{reset\+Visitor}}
\index{reset\+Visitor@{reset\+Visitor}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{reset\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Reset\+Visitor} {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::reset\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_aa2e5c1acffd87cb3cbd82ebcff16bb58}


Locally-\/stored reset visitor. 



Definition at line 302 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!responses@{responses}}
\index{responses@{responses}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{responses}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ arma\+::mat {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::responses\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a4b33d09ea1a798f99311ca25d1ca2769}


The matrix of responses to the input data points. 



Definition at line 275 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!rho@{rho}}
\index{rho@{rho}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{rho}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::rho\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_abcc7132ff103b44854f75f4706e19bad}


Number of steps to backpropagate through time (B\+P\+TT). 



Definition at line 244 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!single@{single}}
\index{single@{single}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{single}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ bool {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::single\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a40cf846d74a3fa9c530902ec8a6776eb}


Only predict the last element of the input sequence. 



Definition at line 266 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!target\+Size@{target\+Size}}
\index{target\+Size@{target\+Size}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{target\+Size}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::target\+Size\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a62f4a2851cf0f88aab7e13de375fcb16}


The target size. 



Definition at line 260 of file rnn.\+hpp.

\index{mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}!weight\+Size\+Visitor@{weight\+Size\+Visitor}}
\index{weight\+Size\+Visitor@{weight\+Size\+Visitor}!mlpack\+::ann\+::\+R\+NN@{mlpack\+::ann\+::\+R\+NN}}
\subsubsection[{weight\+Size\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Output\+Layer\+Type  = Negative\+Log\+Likelihood$<$$>$, typename Initialization\+Rule\+Type  = Random\+Initialization$>$ {\bf Weight\+Size\+Visitor} {\bf mlpack\+::ann\+::\+R\+NN}$<$ Output\+Layer\+Type, Initialization\+Rule\+Type $>$\+::weight\+Size\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RNN_a456c19be7024124922ee318e8aceb993}


Locally-\/stored weight size visitor. 



Definition at line 299 of file rnn.\+hpp.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/mlpack/methods/ann/{\bf rnn.\+hpp}\end{DoxyCompactItemize}
