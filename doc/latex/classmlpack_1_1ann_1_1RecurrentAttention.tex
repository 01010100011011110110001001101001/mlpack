\section{mlpack\+:\+:ann\+:\+:Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1RecurrentAttention}\index{mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$@{mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$}}


This class implements the \doxyref{Recurrent}{p.}{classmlpack_1_1ann_1_1Recurrent} Model for Visual Attention, using a variety of possible layer implementations.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\footnotesize template$<$typename R\+N\+N\+Module\+Type , typename Action\+Module\+Type $>$ }\\{\bf Recurrent\+Attention} (const size\+\_\+t {\bf out\+Size}, const R\+N\+N\+Module\+Type \&rnn, const Action\+Module\+Type \&action, const size\+\_\+t {\bf rho})
\begin{DoxyCompactList}\small\item\em Create the \doxyref{Recurrent\+Attention}{p.}{classmlpack_1_1ann_1_1RecurrentAttention} object using the specified modules. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void {\bf Backward} (const arma\+::\+Mat$<$ eT $>$ \&\&, arma\+::\+Mat$<$ eT $>$ \&\&gy, arma\+::\+Mat$<$ eT $>$ \&\&g)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. \end{DoxyCompactList}\item 
Output\+Data\+Type const \& {\bf Delta} () const 
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
bool {\bf Deterministic} () const 
\begin{DoxyCompactList}\small\item\em The value of the deterministic parameter. \end{DoxyCompactList}\item 
bool \& {\bf Deterministic} ()
\begin{DoxyCompactList}\small\item\em Modify the value of the deterministic parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void {\bf Forward} (arma\+::\+Mat$<$ eT $>$ \&\&input, arma\+::\+Mat$<$ eT $>$ \&\&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void {\bf Gradient} (arma\+::\+Mat$<$ eT $>$ \&\&, arma\+::\+Mat$<$ eT $>$ \&\&, arma\+::\+Mat$<$ eT $>$ \&\&)
\item 
Output\+Data\+Type const \& {\bf Gradient} () const 
\begin{DoxyCompactList}\small\item\em Get the gradient. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Gradient} ()
\begin{DoxyCompactList}\small\item\em Modify the gradient. \end{DoxyCompactList}\item 
Input\+Data\+Type const \& {\bf Input\+Parameter} () const 
\begin{DoxyCompactList}\small\item\em Get the input parameter. \end{DoxyCompactList}\item 
Input\+Data\+Type \& {\bf Input\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the input parameter. \end{DoxyCompactList}\item 
std\+::vector$<$ {\bf Layer\+Types} $>$ \& {\bf Model} ()
\begin{DoxyCompactList}\small\item\em Get the model modules. \end{DoxyCompactList}\item 
Output\+Data\+Type const \& {\bf Output\+Parameter} () const 
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type const \& {\bf Parameters} () const 
\begin{DoxyCompactList}\small\item\em Get the parameters. \end{DoxyCompactList}\item 
Output\+Data\+Type \& {\bf Parameters} ()
\begin{DoxyCompactList}\small\item\em Modify the parameters. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void {\bf Serialize} (Archive \&ar, const unsigned int)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Member Functions}
\begin{DoxyCompactItemize}
\item 
void {\bf Intermediate\+Gradient} ()
\begin{DoxyCompactList}\small\item\em Calculate the gradient of the attention module. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Private Attributes}
\begin{DoxyCompactItemize}
\item 
arma\+::mat {\bf action\+Delta}
\begin{DoxyCompactList}\small\item\em Locally-\/stored action delta. \end{DoxyCompactList}\item 
arma\+::mat {\bf action\+Error}
\begin{DoxyCompactList}\small\item\em Locally-\/stored action error parameter. \end{DoxyCompactList}\item 
{\bf Layer\+Types} {\bf action\+Module}
\begin{DoxyCompactList}\small\item\em Locally-\/stored input module. \end{DoxyCompactList}\item 
arma\+::mat {\bf attention\+Gradient}
\begin{DoxyCompactList}\small\item\em Locally-\/stored attention gradient. \end{DoxyCompactList}\item 
size\+\_\+t {\bf backward\+Step}
\begin{DoxyCompactList}\small\item\em Locally-\/stored number of backward steps. \end{DoxyCompactList}\item 
Output\+Data\+Type {\bf delta}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delta object. \end{DoxyCompactList}\item 
{\bf Delta\+Visitor} {\bf delta\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored delta visitor. \end{DoxyCompactList}\item 
bool {\bf deterministic}
\begin{DoxyCompactList}\small\item\em If true dropout and scaling is disabled, see notes above. \end{DoxyCompactList}\item 
std\+::vector$<$ arma\+::mat $>$ {\bf feedback\+Output\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored feedback output parameters. \end{DoxyCompactList}\item 
size\+\_\+t {\bf forward\+Step}
\begin{DoxyCompactList}\small\item\em Locally-\/stored number of forward steps. \end{DoxyCompactList}\item 
Output\+Data\+Type {\bf gradient}
\begin{DoxyCompactList}\small\item\em Locally-\/stored gradient object. \end{DoxyCompactList}\item 
arma\+::mat {\bf initial\+Input}
\begin{DoxyCompactList}\small\item\em Locally-\/stored initial action input. \end{DoxyCompactList}\item 
{\bf Layer\+Types} {\bf initial\+Module}
\begin{DoxyCompactList}\small\item\em Locally-\/stored initial module. \end{DoxyCompactList}\item 
Input\+Data\+Type {\bf input\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored input parameter object. \end{DoxyCompactList}\item 
arma\+::mat {\bf intermediate\+Gradient}
\begin{DoxyCompactList}\small\item\em Locally-\/stored intermediate gradient for the attention module. \end{DoxyCompactList}\item 
{\bf Layer\+Types} {\bf merge\+Module}
\begin{DoxyCompactList}\small\item\em Locally-\/stored merge module. \end{DoxyCompactList}\item 
std\+::vector$<$ arma\+::mat $>$ {\bf module\+Output\+Parameter}
\begin{DoxyCompactList}\small\item\em List of all module parameters for the backward pass (B\+B\+TT). \end{DoxyCompactList}\item 
std\+::vector$<$ {\bf Layer\+Types} $>$ {\bf network}
\begin{DoxyCompactList}\small\item\em Locally-\/stored model modules. \end{DoxyCompactList}\item 
Output\+Data\+Type {\bf output\+Parameter}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output parameter object. \end{DoxyCompactList}\item 
{\bf Output\+Parameter\+Visitor} {\bf output\+Parameter\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored output parameter visitor. \end{DoxyCompactList}\item 
size\+\_\+t {\bf out\+Size}
\begin{DoxyCompactList}\small\item\em Locally-\/stored module output size. \end{DoxyCompactList}\item 
Output\+Data\+Type {\bf parameters}
\begin{DoxyCompactList}\small\item\em Locally-\/stored weight object. \end{DoxyCompactList}\item 
arma\+::mat {\bf recurrent\+Error}
\begin{DoxyCompactList}\small\item\em Locally-\/stored recurrent error parameter. \end{DoxyCompactList}\item 
{\bf Layer\+Types} {\bf recurrent\+Module}
\begin{DoxyCompactList}\small\item\em Locally-\/stored recurrent module. \end{DoxyCompactList}\item 
{\bf Reset\+Visitor} {\bf reset\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored reset visitor. \end{DoxyCompactList}\item 
size\+\_\+t {\bf rho}
\begin{DoxyCompactList}\small\item\em Number of steps to backpropagate through time (B\+P\+TT). \end{DoxyCompactList}\item 
arma\+::mat {\bf rnn\+Delta}
\begin{DoxyCompactList}\small\item\em Locally-\/stored recurrent delta. \end{DoxyCompactList}\item 
{\bf Layer\+Types} {\bf rnn\+Module}
\begin{DoxyCompactList}\small\item\em Locally-\/stored start module. \end{DoxyCompactList}\item 
{\bf Weight\+Size\+Visitor} {\bf weight\+Size\+Visitor}
\begin{DoxyCompactList}\small\item\em Locally-\/stored weight size visitor. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\\*
class mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

This class implements the \doxyref{Recurrent}{p.}{classmlpack_1_1ann_1_1Recurrent} Model for Visual Attention, using a variety of possible layer implementations. 

For more information, see the following paper.


\begin{DoxyCode}
@article\{MnihHGK14,
  title=\{Recurrent Models of Visual Attention\},
  author=\{Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu\},
  journal=\{CoRR\},
  volume=\{abs/1406.6247\},
  year=\{2014\}
\}
\end{DoxyCode}



\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 76 of file layer\+\_\+types.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Recurrent\+Attention@{Recurrent\+Attention}}
\index{Recurrent\+Attention@{Recurrent\+Attention}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Recurrent\+Attention(const size\+\_\+t out\+Size, const R\+N\+N\+Module\+Type \&rnn, const Action\+Module\+Type \&action, const size\+\_\+t rho)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename R\+N\+N\+Module\+Type , typename Action\+Module\+Type $>$ {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::{\bf Recurrent\+Attention} (
\begin{DoxyParamCaption}
\item[{const size\+\_\+t}]{out\+Size, }
\item[{const R\+N\+N\+Module\+Type \&}]{rnn, }
\item[{const Action\+Module\+Type \&}]{action, }
\item[{const size\+\_\+t}]{rho}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RecurrentAttention_a19f682d512f398ca875a91dba8ed8692}


Create the \doxyref{Recurrent\+Attention}{p.}{classmlpack_1_1ann_1_1RecurrentAttention} object using the specified modules. 


\begin{DoxyParams}{Parameters}
{\em start} & The module output size. \\
\hline
{\em start} & The recurrent neural network module. \\
\hline
{\em start} & The action module. \\
\hline
{\em rho} & Maximum number of steps to backpropagate through time (B\+P\+TT). \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Backward(const arma\+::\+Mat$<$ e\+T $>$ \&\&, arma\+::\+Mat$<$ e\+T $>$ \&\&gy, arma\+::\+Mat$<$ e\+T $>$ \&\&g)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename eT $>$ void {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Backward (
\begin{DoxyParamCaption}
\item[{const arma\+::\+Mat$<$ eT $>$ \&\&}]{, }
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{gy, }
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{g}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RecurrentAttention_a0755012425c0a71ce6b4ce6381207650}


Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. 

Using the results from the feed forward pass.


\begin{DoxyParams}{Parameters}
{\em input} & The propagated input activation. \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Delta() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type const\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Delta (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_aa6ba8ecdce51104b9448121b047cc682}


Get the delta. 



Definition at line 132 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Delta()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Delta (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a5a0314639c2ebd6e6dc6cab29139ab49}


Modify the delta. 



Definition at line 134 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Deterministic() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ bool {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Deterministic (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ac1e14d38436eb66b0a65710848b1b9be}


The value of the deterministic parameter. 



Definition at line 112 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::deterministic.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Deterministic()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ bool\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Deterministic (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a5203a680eb6615ecf60ea59db7528c0d}


Modify the value of the deterministic parameter. 



Definition at line 114 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::deterministic.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Forward(arma\+::\+Mat$<$ e\+T $>$ \&\&input, arma\+::\+Mat$<$ e\+T $>$ \&\&output)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename eT $>$ void {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Forward (
\begin{DoxyParamCaption}
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{input, }
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{output}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RecurrentAttention_aa7a3dfdfe84b6a15776f1c8257b4956a}


Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data used for evaluating the specified function. \\
\hline
{\em output} & Resulting output activation. \\
\hline
\end{DoxyParams}
\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Gradient(arma\+::\+Mat$<$ e\+T $>$ \&\&, arma\+::\+Mat$<$ e\+T $>$ \&\&, arma\+::\+Mat$<$ e\+T $>$ \&\&)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename eT $>$ void {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{, }
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{, }
\item[{arma\+::\+Mat$<$ eT $>$ \&\&}]{}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RecurrentAttention_a238175271d9dda33b397986947cb7950}
\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Gradient() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type const\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ae9d06c675ca35bebef0695ea2be9420b}


Get the gradient. 



Definition at line 137 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::gradient.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Gradient()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Gradient (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a8ad9522d1ae92f2b12e25af90b8e2adc}


Modify the gradient. 



Definition at line 139 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::gradient, and mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Serialize().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Input\+Parameter@{Input\+Parameter}}
\index{Input\+Parameter@{Input\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Input\+Parameter() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Input\+Data\+Type const\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Input\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a362e8cfdc65bb93f1c42d151aeb285b1}


Get the input parameter. 



Definition at line 122 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::input\+Parameter.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Input\+Parameter@{Input\+Parameter}}
\index{Input\+Parameter@{Input\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Input\+Parameter()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Input\+Data\+Type\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Input\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a090a6ff673e3cb209ca87711dfca899f}


Modify the input parameter. 



Definition at line 124 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::input\+Parameter.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Intermediate\+Gradient@{Intermediate\+Gradient}}
\index{Intermediate\+Gradient@{Intermediate\+Gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Intermediate\+Gradient()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ void {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Intermediate\+Gradient (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a39dee24af36a901e09bbbd37d1ae5da4}


Calculate the gradient of the attention module. 



Definition at line 149 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::action\+Error, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::action\+Module, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::attention\+Gradient, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::backward\+Step, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::initial\+Input, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::intermediate\+Gradient, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter\+Visitor, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::recurrent\+Error, mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::rho, and mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::rnn\+Module.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Model@{Model}}
\index{Model@{Model}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Model()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ std\+::vector$<${\bf Layer\+Types}$>$\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Model (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ac4148977cfbd072efc7c650ba99113af}


Get the model modules. 



Definition at line 109 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::network.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Output\+Parameter() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type const\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Output\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_abfb656c46c9ee91aef52653d2d3b03ab}


Get the output parameter. 



Definition at line 127 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Output\+Parameter()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Output\+Parameter (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a1fdd26eb5c41cae15afddd82a1fa93e7}


Modify the output parameter. 



Definition at line 129 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Parameters() const }]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type const\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
) const\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_aaebc417fc7f43c6648eea3f034e43456}


Get the parameters. 



Definition at line 117 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::parameters.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Parameters()}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type\& {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Parameters (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a1c3e7cd936a337cbd8fed60450660f92}


Modify the parameters. 



Definition at line 119 of file recurrent\+\_\+attention.\+hpp.



References mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::parameters.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!Serialize@{Serialize}}
\index{Serialize@{Serialize}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{Serialize(\+Archive \&ar, const unsigned int)}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ template$<$typename Archive $>$ void {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::Serialize (
\begin{DoxyParamCaption}
\item[{Archive \&}]{ar, }
\item[{const unsigned}]{int}
\end{DoxyParamCaption}
)}\label{classmlpack_1_1ann_1_1RecurrentAttention_accd62dfa0e1fef09b8cbd3f88b6aaa4c}


Serialize the layer. 



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Gradient().



\subsection{Member Data Documentation}
\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!action\+Delta@{action\+Delta}}
\index{action\+Delta@{action\+Delta}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{action\+Delta}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::action\+Delta\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a71e9916813df3cd97b852b7aca842e8f}


Locally-\/stored action delta. 



Definition at line 244 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!action\+Error@{action\+Error}}
\index{action\+Error@{action\+Error}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{action\+Error}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::action\+Error\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ab7d7d894a275bed0b00d183320a6be65}


Locally-\/stored action error parameter. 



Definition at line 241 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!action\+Module@{action\+Module}}
\index{action\+Module@{action\+Module}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{action\+Module}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Layer\+Types} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::action\+Module\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ab6c4e98468d2cf837212416d5961a0cd}


Locally-\/stored input module. 



Definition at line 181 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!attention\+Gradient@{attention\+Gradient}}
\index{attention\+Gradient@{attention\+Gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{attention\+Gradient}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::attention\+Gradient\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a24ea3d5f07b29367bca715c42b6c6cf4}


Locally-\/stored attention gradient. 



Definition at line 256 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!backward\+Step@{backward\+Step}}
\index{backward\+Step@{backward\+Step}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{backward\+Step}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::backward\+Step\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a1b30f13a0fe58eccf60e016d7c91a468}


Locally-\/stored number of backward steps. 



Definition at line 190 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!delta@{delta}}
\index{delta@{delta}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{delta}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a1f736747773ee40ca22a971193500cdf}


Locally-\/stored delta object. 



Definition at line 226 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Delta().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!delta\+Visitor@{delta\+Visitor}}
\index{delta\+Visitor@{delta\+Visitor}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{delta\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Delta\+Visitor} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::delta\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a96f330bd8a403a509ef32b2866cb03da}


Locally-\/stored delta visitor. 



Definition at line 214 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!deterministic@{deterministic}}
\index{deterministic@{deterministic}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{deterministic}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ bool {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::deterministic\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a77b811d4b1991f8928d8a1f39e911048}


If true dropout and scaling is disabled, see notes above. 



Definition at line 193 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Deterministic().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!feedback\+Output\+Parameter@{feedback\+Output\+Parameter}}
\index{feedback\+Output\+Parameter@{feedback\+Output\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{feedback\+Output\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ std\+::vector$<$arma\+::mat$>$ {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::feedback\+Output\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a7d007c1d41cc9894fcd45075f8815d26}


Locally-\/stored feedback output parameters. 



Definition at line 220 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!forward\+Step@{forward\+Step}}
\index{forward\+Step@{forward\+Step}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{forward\+Step}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::forward\+Step\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a7eaf0aff785c43278adb77e11fc4141f}


Locally-\/stored number of forward steps. 



Definition at line 187 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!gradient@{gradient}}
\index{gradient@{gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{gradient}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::gradient\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a2c0e2e07af58601c50749b1046d80973}


Locally-\/stored gradient object. 



Definition at line 229 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!initial\+Input@{initial\+Input}}
\index{initial\+Input@{initial\+Input}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{initial\+Input}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::initial\+Input\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_aae06e82d6c78e58e411138d9158c3e1b}


Locally-\/stored initial action input. 



Definition at line 250 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!initial\+Module@{initial\+Module}}
\index{initial\+Module@{initial\+Module}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{initial\+Module}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Layer\+Types} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::initial\+Module\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ade55fa8342c9bcae3ae77014b01606a7}


Locally-\/stored initial module. 



Definition at line 199 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!input\+Parameter@{input\+Parameter}}
\index{input\+Parameter@{input\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{input\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Input\+Data\+Type {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::input\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a85cfb3c933cc1e4dd5c0c71417c59a25}


Locally-\/stored input parameter object. 



Definition at line 232 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Input\+Parameter().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!intermediate\+Gradient@{intermediate\+Gradient}}
\index{intermediate\+Gradient@{intermediate\+Gradient}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{intermediate\+Gradient}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::intermediate\+Gradient\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a47f4316b9656db9830d27db4ef7dd79a}


Locally-\/stored intermediate gradient for the attention module. 



Definition at line 259 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!merge\+Module@{merge\+Module}}
\index{merge\+Module@{merge\+Module}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{merge\+Module}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Layer\+Types} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::merge\+Module\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ad351382c3727ae0a77e78d6f99f734f5}


Locally-\/stored merge module. 



Definition at line 208 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!module\+Output\+Parameter@{module\+Output\+Parameter}}
\index{module\+Output\+Parameter@{module\+Output\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{module\+Output\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ std\+::vector$<$arma\+::mat$>$ {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::module\+Output\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ab4f862c30e171f6653b643d5e6647a85}


List of all module parameters for the backward pass (B\+B\+TT). 



Definition at line 223 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!network@{network}}
\index{network@{network}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{network}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ std\+::vector$<${\bf Layer\+Types}$>$ {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::network\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_aab4436c184e8d3d9c92836140b6e7b23}


Locally-\/stored model modules. 



Definition at line 205 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Model().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!output\+Parameter@{output\+Parameter}}
\index{output\+Parameter@{output\+Parameter}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{output\+Parameter}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a746f720a444112d3505aa2c51605f044}


Locally-\/stored output parameter object. 



Definition at line 235 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Output\+Parameter().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!output\+Parameter\+Visitor@{output\+Parameter\+Visitor}}
\index{output\+Parameter\+Visitor@{output\+Parameter\+Visitor}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{output\+Parameter\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Output\+Parameter\+Visitor} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::output\+Parameter\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a06fe2fba7ec7dbf60cb6de583fceb472}


Locally-\/stored output parameter visitor. 



Definition at line 217 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!out\+Size@{out\+Size}}
\index{out\+Size@{out\+Size}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{out\+Size}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::out\+Size\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_ad17b0815047347a9df76e2787d5dafa2}


Locally-\/stored module output size. 



Definition at line 175 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!parameters@{parameters}}
\index{parameters@{parameters}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{parameters}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ Output\+Data\+Type {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::parameters\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a590df1e786e3189b7e8c671fa7bc881c}


Locally-\/stored weight object. 



Definition at line 196 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Parameters().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!recurrent\+Error@{recurrent\+Error}}
\index{recurrent\+Error@{recurrent\+Error}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{recurrent\+Error}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::recurrent\+Error\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a7b49e1a16716affeca8775b0eebf735f}


Locally-\/stored recurrent error parameter. 



Definition at line 238 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!recurrent\+Module@{recurrent\+Module}}
\index{recurrent\+Module@{recurrent\+Module}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{recurrent\+Module}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Layer\+Types} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::recurrent\+Module\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a088da132402dcf93478b20a127b999c3}


Locally-\/stored recurrent module. 



Definition at line 202 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!reset\+Visitor@{reset\+Visitor}}
\index{reset\+Visitor@{reset\+Visitor}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{reset\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Reset\+Visitor} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::reset\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a254ceb823abc1b36daee61038254a5c0}


Locally-\/stored reset visitor. 



Definition at line 253 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!rho@{rho}}
\index{rho@{rho}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{rho}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ size\+\_\+t {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::rho\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_aa14155589771e3e9a78c0493e90c064d}


Number of steps to backpropagate through time (B\+P\+TT). 



Definition at line 184 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!rnn\+Delta@{rnn\+Delta}}
\index{rnn\+Delta@{rnn\+Delta}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{rnn\+Delta}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ arma\+::mat {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::rnn\+Delta\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a9f177b1a1cdf09b7205b6c6e154a8a41}


Locally-\/stored recurrent delta. 



Definition at line 247 of file recurrent\+\_\+attention.\+hpp.

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!rnn\+Module@{rnn\+Module}}
\index{rnn\+Module@{rnn\+Module}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{rnn\+Module}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Layer\+Types} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::rnn\+Module\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a239209777c3d9d1a582f23abf19f3fc4}


Locally-\/stored start module. 



Definition at line 178 of file recurrent\+\_\+attention.\+hpp.



Referenced by mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Intermediate\+Gradient().

\index{mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}!weight\+Size\+Visitor@{weight\+Size\+Visitor}}
\index{weight\+Size\+Visitor@{weight\+Size\+Visitor}!mlpack\+::ann\+::\+Recurrent\+Attention@{mlpack\+::ann\+::\+Recurrent\+Attention}}
\subsubsection[{weight\+Size\+Visitor}]{\setlength{\rightskip}{0pt plus 5cm}template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ {\bf Weight\+Size\+Visitor} {\bf mlpack\+::ann\+::\+Recurrent\+Attention}$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::weight\+Size\+Visitor\hspace{0.3cm}{\ttfamily [private]}}\label{classmlpack_1_1ann_1_1RecurrentAttention_a33034968bc81db083711bac84dc4a1cf}


Locally-\/stored weight size visitor. 



Definition at line 211 of file recurrent\+\_\+attention.\+hpp.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
src/mlpack/methods/ann/layer/{\bf layer\+\_\+types.\+hpp}\item 
src/mlpack/methods/ann/layer/{\bf recurrent\+\_\+attention.\+hpp}\end{DoxyCompactItemize}
