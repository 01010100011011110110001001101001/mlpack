\section{src/mlpack/core/optimizers/adam/adam.hpp File Reference}
\label{adam_8hpp}\index{src/mlpack/core/optimizers/adam/adam.\+hpp@{src/mlpack/core/optimizers/adam/adam.\+hpp}}
Include dependency graph for adam.\+hpp\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{adam_8hpp__incl}
\end{center}
\end{figure}
\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class {\bf mlpack\+::optimization\+::\+Adam$<$ Decomposable\+Function\+Type $>$}
\begin{DoxyCompactList}\small\item\em \doxyref{Adam}{p.}{classmlpack_1_1optimization_1_1Adam} is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Namespaces}
\begin{DoxyCompactItemize}
\item 
 {\bf mlpack}
\begin{DoxyCompactList}\small\item\em Linear algebra utility functions, generally performed on matrices or vectors. \end{DoxyCompactList}\item 
 {\bf mlpack\+::optimization}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyAuthor}{Author}
Ryan Curtin 

Vasanth Kalingeri 

Marcus Edel 

Vivek Pal
\end{DoxyAuthor}
Adam and Ada\+Max optimizer. Adam is an an algorithm for first-\/order gradient-\/ -\/based optimization of stochastic objective functions, based on adaptive estimates of lower-\/order moments. Ada\+Max is simply a variant of Adam based on the infinity norm.

mlpack is free software; you may redistribute it and/or modify it under the terms of the 3-\/clause B\+SD license. You should have received a copy of the 3-\/clause B\+SD license along with mlpack. If not, see {\tt http\+://www.\+opensource.\+org/licenses/\+B\+S\+D-\/3-\/\+Clause} for more information. 