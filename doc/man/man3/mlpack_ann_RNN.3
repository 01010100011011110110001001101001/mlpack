.TH "mlpack::ann::RNN< OutputLayerType, InitializationRuleType >" 3 "Sat Mar 25 2017" "Version master" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
mlpack::ann::RNN< OutputLayerType, InitializationRuleType > \- Implementation of a standard recurrent neural network container\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Types"

.in +1c
.ti -1c
.RI "using \fBNetworkType\fP = \fBRNN\fP< OutputLayerType, InitializationRuleType >"
.br
.RI "\fIConvenience typedef for the internal model construction\&. \fP"
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBRNN\fP (const size_t \fBrho\fP, const bool \fBsingle\fP=false, OutputLayerType \fBoutputLayer\fP=OutputLayerType(), InitializationRuleType \fBinitializeRule\fP=InitializationRuleType())"
.br
.RI "\fICreate the \fBRNN\fP object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer\&. \fP"
.ti -1c
.RI "\fBRNN\fP (const arma::mat &\fBpredictors\fP, const arma::mat &\fBresponses\fP, const size_t \fBrho\fP, const bool \fBsingle\fP=false, OutputLayerType \fBoutputLayer\fP=OutputLayerType(), InitializationRuleType \fBinitializeRule\fP=InitializationRuleType())"
.br
.RI "\fICreate the \fBRNN\fP object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer\&. \fP"
.ti -1c
.RI "\fB~RNN\fP ()"
.br
.RI "\fIDestructor to release allocated memory\&. \fP"
.ti -1c
.RI "template<typename LayerType > void \fBAdd\fP (const LayerType &layer)"
.br
.ti -1c
.RI "template<class LayerType , class\&.\&.\&. Args> void \fBAdd\fP (Args\&.\&.\&.args)"
.br
.ti -1c
.RI "void \fBAdd\fP (\fBLayerTypes\fP layer)"
.br
.ti -1c
.RI "double \fBEvaluate\fP (const arma::mat &, const size_t i, const bool \fBdeterministic\fP=true)"
.br
.RI "\fIEvaluate the recurrent neural network with the given parameters\&. \fP"
.ti -1c
.RI "void \fBGradient\fP (const arma::mat &parameters, const size_t i, arma::mat &gradient)"
.br
.RI "\fIEvaluate the gradient of the recurrent neural network with the given parameters, and with respect to only one point in the dataset\&. \fP"
.ti -1c
.RI "size_t \fBNumFunctions\fP () const "
.br
.RI "\fIReturn the number of separable functions (the number of predictor points)\&. \fP"
.ti -1c
.RI "const arma::mat & \fBParameters\fP () const "
.br
.RI "\fIReturn the initial point for the optimization\&. \fP"
.ti -1c
.RI "arma::mat & \fBParameters\fP ()"
.br
.RI "\fIModify the initial point for the optimization\&. \fP"
.ti -1c
.RI "void \fBPredict\fP (arma::mat &\fBpredictors\fP, arma::mat &\fBresponses\fP)"
.br
.RI "\fIPredict the responses to a given set of predictors\&. \fP"
.ti -1c
.RI "template<typename Archive > void \fBSerialize\fP (Archive &ar, const unsigned int)"
.br
.RI "\fISerialize the model\&. \fP"
.ti -1c
.RI "template<template< typename > class OptimizerType = mlpack::optimization::StandardSGD> void \fBTrain\fP (const arma::mat &\fBpredictors\fP, const arma::mat &\fBresponses\fP, OptimizerType< \fBNetworkType\fP > &optimizer)"
.br
.RI "\fITrain the recurrent neural network on the given input data using the given optimizer\&. \fP"
.ti -1c
.RI "template<template< typename > class OptimizerType = mlpack::optimization::StandardSGD> void \fBTrain\fP (const arma::mat &\fBpredictors\fP, const arma::mat &\fBresponses\fP)"
.br
.RI "\fITrain the recurrent neural network on the given input data\&. \fP"
.in -1c
.SS "Private Member Functions"

.in +1c
.ti -1c
.RI "void \fBBackward\fP ()"
.br
.RI "\fIThe Backward algorithm (part of the Forward-Backward algorithm)\&. \fP"
.ti -1c
.RI "void \fBForward\fP (arma::mat &&input)"
.br
.RI "\fIThe Forward algorithm (part of the Forward-Backward algorithm)\&. \fP"
.ti -1c
.RI "void \fBGradient\fP ()"
.br
.RI "\fIIterate through all layer modules and update the the gradient using the layer defined optimizer\&. \fP"
.ti -1c
.RI "void \fBResetDeterministic\fP ()"
.br
.RI "\fIReset the module status by setting the current deterministic parameter for all modules that implement the Deterministic function\&. \fP"
.ti -1c
.RI "void \fBResetGradients\fP (arma::mat &gradient)"
.br
.RI "\fIReset the gradient for all modules that implement the Gradient function\&. \fP"
.ti -1c
.RI "void \fBResetParameters\fP ()"
.br
.RI "\fIReset the module infomration (weights/parameters)\&. \fP"
.ti -1c
.RI "void \fBSinglePredict\fP (const arma::mat &\fBpredictors\fP, arma::mat &\fBresponses\fP)"
.br
.in -1c
.SS "Private Attributes"

.in +1c
.ti -1c
.RI "arma::mat \fBcurrentInput\fP"
.br
.RI "\fITHe current input of the forward/backward pass\&. \fP"
.ti -1c
.RI "\fBDeleteVisitor\fP \fBdeleteVisitor\fP"
.br
.RI "\fILocally-stored delete visitor\&. \fP"
.ti -1c
.RI "\fBDeltaVisitor\fP \fBdeltaVisitor\fP"
.br
.RI "\fILocally-stored delta visitor\&. \fP"
.ti -1c
.RI "bool \fBdeterministic\fP"
.br
.RI "\fIThe current evaluation mode (training or testing)\&. \fP"
.ti -1c
.RI "arma::mat \fBerror\fP"
.br
.RI "\fIThe current error for the backward pass\&. \fP"
.ti -1c
.RI "InitializationRuleType \fBinitializeRule\fP"
.br
.RI "\fIInstantiated InitializationRule object for initializing the network parameter\&. \fP"
.ti -1c
.RI "size_t \fBinputSize\fP"
.br
.RI "\fIThe input size\&. \fP"
.ti -1c
.RI "std::vector< arma::mat > \fBmoduleOutputParameter\fP"
.br
.RI "\fIList of all module parameters for the backward pass (BBTT)\&. \fP"
.ti -1c
.RI "std::vector< \fBLayerTypes\fP > \fBnetwork\fP"
.br
.RI "\fILocally-stored model modules\&. \fP"
.ti -1c
.RI "size_t \fBnumFunctions\fP"
.br
.RI "\fIThe number of separable functions (the number of predictor points)\&. \fP"
.ti -1c
.RI "OutputLayerType \fBoutputLayer\fP"
.br
.RI "\fIInstantiated outputlayer used to evaluate the network\&. \fP"
.ti -1c
.RI "\fBOutputParameterVisitor\fP \fBoutputParameterVisitor\fP"
.br
.RI "\fILocally-stored output parameter visitor\&. \fP"
.ti -1c
.RI "size_t \fBoutputSize\fP"
.br
.RI "\fIThe output size\&. \fP"
.ti -1c
.RI "arma::mat \fBparameter\fP"
.br
.RI "\fIMatrix of (trained) parameters\&. \fP"
.ti -1c
.RI "arma::mat \fBpredictors\fP"
.br
.RI "\fIThe matrix of data points (predictors)\&. \fP"
.ti -1c
.RI "bool \fBreset\fP"
.br
.RI "\fIIndicator if we already trained the model\&. \fP"
.ti -1c
.RI "\fBResetVisitor\fP \fBresetVisitor\fP"
.br
.RI "\fILocally-stored reset visitor\&. \fP"
.ti -1c
.RI "arma::mat \fBresponses\fP"
.br
.RI "\fIThe matrix of responses to the input data points\&. \fP"
.ti -1c
.RI "size_t \fBrho\fP"
.br
.RI "\fINumber of steps to backpropagate through time (BPTT)\&. \fP"
.ti -1c
.RI "bool \fBsingle\fP"
.br
.RI "\fIOnly predict the last element of the input sequence\&. \fP"
.ti -1c
.RI "size_t \fBtargetSize\fP"
.br
.RI "\fIThe target size\&. \fP"
.ti -1c
.RI "\fBWeightSizeVisitor\fP \fBweightSizeVisitor\fP"
.br
.RI "\fILocally-stored weight size visitor\&. \fP"
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename OutputLayerType = NegativeLogLikelihood<>, typename InitializationRuleType = RandomInitialization>
.br
class mlpack::ann::RNN< OutputLayerType, InitializationRuleType >"
Implementation of a standard recurrent neural network container\&. 


.PP
\fBTemplate Parameters:\fP
.RS 4
\fIOutputLayerType\fP The output layer type used to evaluate the network\&. 
.br
\fIInitializationRuleType\fP Rule used to initialize the weight matrix\&. 
.RE
.PP

.PP
Definition at line 40 of file rnn\&.hpp\&.
.SH "Member Typedef Documentation"
.PP 
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> using \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::\fBNetworkType\fP =  \fBRNN\fP<OutputLayerType, InitializationRuleType>"

.PP
Convenience typedef for the internal model construction\&. 
.PP
Definition at line 44 of file rnn\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::\fBRNN\fP (const size_t rho, const bool single = \fCfalse\fP, OutputLayerType outputLayer = \fCOutputLayerType()\fP, InitializationRuleType initializeRule = \fCInitializationRuleType()\fP)"

.PP
Create the \fBRNN\fP object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer\&. Optionally, specify which initialize rule and performance function should be used\&.
.PP
\fBParameters:\fP
.RS 4
\fIrho\fP Maximum number of steps to backpropagate through time (BPTT)\&. 
.br
\fIsingle\fP Predict only the last element of the input sequence\&. 
.br
\fIoutputLayer\fP Output layer used to evaluate the network\&. 
.br
\fIinitializeRule\fP Optional instantiated InitializationRule object for initializing the network parameter\&. 
.RE
.PP

.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::\fBRNN\fP (const arma::mat & predictors, const arma::mat & responses, const size_t rho, const bool single = \fCfalse\fP, OutputLayerType outputLayer = \fCOutputLayerType()\fP, InitializationRuleType initializeRule = \fCInitializationRuleType()\fP)"

.PP
Create the \fBRNN\fP object with the given predictors and responses set (this is the set that is used to train the network) and the given optimizer\&. Optionally, specify which initialize rule and performance function should be used\&.
.PP
\fBParameters:\fP
.RS 4
\fIpredictors\fP Input training variables\&. 
.br
\fIresponses\fP Outputs results from input training variables\&. 
.br
\fIrho\fP Maximum number of steps to backpropagate through time (BPTT)\&. 
.br
\fIsingle\fP Predict only the last element of the input sequence\&. 
.br
\fIoutputLayer\fP Output layer used to evaluate the network\&. 
.br
\fIinitializeRule\fP Optional instantiated InitializationRule object for initializing the network parameter\&. 
.RE
.PP

.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::~\fBRNN\fP ()"

.PP
Destructor to release allocated memory\&. 
.SH "Member Function Documentation"
.PP 
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> template<typename LayerType > void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::\fBAdd\fP (const LayerType & layer)\fC [inline]\fP"

.PP
Definition at line 168 of file rnn\&.hpp\&.
.PP
References mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::network\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> template<class LayerType , class\&.\&.\&. Args> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::\fBAdd\fP (Args\&.\&.\&. args)\fC [inline]\fP"

.PP
Definition at line 176 of file rnn\&.hpp\&.
.PP
References mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::network\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::\fBAdd\fP (\fBLayerTypes\fP layer)\fC [inline]\fP"

.PP
Definition at line 183 of file rnn\&.hpp\&.
.PP
References mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::network\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Backward ()\fC [private]\fP"

.PP
The Backward algorithm (part of the Forward-Backward algorithm)\&. Computes backward pass for module\&. 
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> double \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Evaluate (const arma::mat &, const size_t i, const bool deterministic = \fCtrue\fP)"

.PP
Evaluate the recurrent neural network with the given parameters\&. This function is usually called by the optimizer to train the model\&.
.PP
\fBParameters:\fP
.RS 4
\fIparameters\fP Matrix model parameters\&. 
.br
\fIi\fP Index of point to use for objective function evaluation\&. 
.br
\fIdeterministic\fP Whether or not to train or test the model\&. Note some layer act differently in training or testing mode\&. 
.RE
.PP

.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Forward (arma::mat && input)\fC [private]\fP"

.PP
The Forward algorithm (part of the Forward-Backward algorithm)\&. Computes forward probabilities for each module\&.
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP Data sequence to compute probabilities for\&. 
.RE
.PP

.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Gradient (const arma::mat & parameters, const size_t i, arma::mat & gradient)"

.PP
Evaluate the gradient of the recurrent neural network with the given parameters, and with respect to only one point in the dataset\&. This is useful for optimizers such as SGD, which require a separable objective function\&.
.PP
\fBParameters:\fP
.RS 4
\fIparameters\fP Matrix of the model parameters to be optimized\&. 
.br
\fIi\fP Index of points to use for objective function gradient evaluation\&. 
.br
\fIgradient\fP Matrix to output gradient into\&. 
.RE
.PP

.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Gradient ()\fC [private]\fP"

.PP
Iterate through all layer modules and update the the gradient using the layer defined optimizer\&. 
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> size_t \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::NumFunctions () const\fC [inline]\fP"

.PP
Return the number of separable functions (the number of predictor points)\&. 
.PP
Definition at line 186 of file rnn\&.hpp\&.
.PP
References mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::numFunctions\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> const arma::mat& \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Parameters () const\fC [inline]\fP"

.PP
Return the initial point for the optimization\&. 
.PP
Definition at line 189 of file rnn\&.hpp\&.
.PP
References mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::parameter\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> arma::mat& \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Parameters ()\fC [inline]\fP"

.PP
Modify the initial point for the optimization\&. 
.PP
Definition at line 191 of file rnn\&.hpp\&.
.PP
References mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Backward(), mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Forward(), mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Gradient(), mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::parameter, mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::ResetDeterministic(), mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::ResetGradients(), mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::ResetParameters(), mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Serialize(), and mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::SinglePredict()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Predict (arma::mat & predictors, arma::mat & responses)"

.PP
Predict the responses to a given set of predictors\&. The responses will reflect the output of the given output layer as returned by the output layer function\&.
.PP
\fBParameters:\fP
.RS 4
\fIpredictors\fP Input predictors\&. 
.br
\fIresponses\fP Matrix to put output predictions of responses into\&. 
.RE
.PP

.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::ResetDeterministic ()\fC [private]\fP"

.PP
Reset the module status by setting the current deterministic parameter for all modules that implement the Deterministic function\&. 
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::ResetGradients (arma::mat & gradient)\fC [private]\fP"

.PP
Reset the gradient for all modules that implement the Gradient function\&. 
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::ResetParameters ()\fC [private]\fP"

.PP
Reset the module infomration (weights/parameters)\&. 
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> template<typename Archive > void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Serialize (Archive & ar, const unsigned int)"

.PP
Serialize the model\&. 
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::SinglePredict (const arma::mat & predictors, arma::mat & responses)\fC [private]\fP"

.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> template<template< typename > class OptimizerType = mlpack::optimization::StandardSGD> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Train (const arma::mat & predictors, const arma::mat & responses, OptimizerType< \fBNetworkType\fP > & optimizer)"

.PP
Train the recurrent neural network on the given input data using the given optimizer\&. This will use the existing model parameters as a starting point for the optimization\&. If this is not what you want, then you should access the parameters vector directly with \fBParameters()\fP and modify it as desired\&.
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIOptimizerType\fP Type of optimizer to use to train the model\&. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIpredictors\fP Input training variables\&. 
.br
\fIresponses\fP Outputs results from input training variables\&. 
.br
\fIoptimizer\fP Instantiated optimizer used to train the model\&. 
.RE
.PP

.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> template<template< typename > class OptimizerType = mlpack::optimization::StandardSGD> void \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::Train (const arma::mat & predictors, const arma::mat & responses)"

.PP
Train the recurrent neural network on the given input data\&. By default, the SGD optimization algorithm is used, but others can be specified (such as \fBmlpack::optimization::RMSprop\fP)\&.
.PP
This will use the existing model parameters as a starting point for the optimization\&. If this is not what you want, then you should access the parameters vector directly with \fBParameters()\fP and modify it as desired\&.
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIOptimizerType\fP Type of optimizer to use to train the model\&. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIpredictors\fP Input training variables\&. 
.br
\fIresponses\fP Outputs results from input training variables\&. 
.RE
.PP

.SH "Member Data Documentation"
.PP 
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> arma::mat \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::currentInput\fC [private]\fP"

.PP
THe current input of the forward/backward pass\&. 
.PP
Definition at line 287 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBDeleteVisitor\fP \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::deleteVisitor\fC [private]\fP"

.PP
Locally-stored delete visitor\&. 
.PP
Definition at line 305 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBDeltaVisitor\fP \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::deltaVisitor\fC [private]\fP"

.PP
Locally-stored delta visitor\&. 
.PP
Definition at line 290 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> bool \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::deterministic\fC [private]\fP"

.PP
The current evaluation mode (training or testing)\&. 
.PP
Definition at line 308 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> arma::mat \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::error\fC [private]\fP"

.PP
The current error for the backward pass\&. 
.PP
Definition at line 284 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> InitializationRuleType \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::initializeRule\fC [private]\fP"

.PP
Instantiated InitializationRule object for initializing the network parameter\&. 
.PP
Definition at line 251 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> size_t \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::inputSize\fC [private]\fP"

.PP
The input size\&. 
.PP
Definition at line 254 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> std::vector<arma::mat> \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::moduleOutputParameter\fC [private]\fP"

.PP
List of all module parameters for the backward pass (BBTT)\&. 
.PP
Definition at line 296 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> std::vector<\fBLayerTypes\fP> \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::network\fC [private]\fP"

.PP
Locally-stored model modules\&. 
.PP
Definition at line 269 of file rnn\&.hpp\&.
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Add()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> size_t \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::numFunctions\fC [private]\fP"

.PP
The number of separable functions (the number of predictor points)\&. 
.PP
Definition at line 281 of file rnn\&.hpp\&.
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::NumFunctions()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> OutputLayerType \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::outputLayer\fC [private]\fP"

.PP
Instantiated outputlayer used to evaluate the network\&. 
.PP
Definition at line 247 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBOutputParameterVisitor\fP \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::outputParameterVisitor\fC [private]\fP"

.PP
Locally-stored output parameter visitor\&. 
.PP
Definition at line 293 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> size_t \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::outputSize\fC [private]\fP"

.PP
The output size\&. 
.PP
Definition at line 257 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> arma::mat \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::parameter\fC [private]\fP"

.PP
Matrix of (trained) parameters\&. 
.PP
Definition at line 278 of file rnn\&.hpp\&.
.PP
Referenced by mlpack::ann::RNN< OutputLayerType, InitializationRuleType >::Parameters()\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> arma::mat \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::predictors\fC [private]\fP"

.PP
The matrix of data points (predictors)\&. 
.PP
Definition at line 272 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> bool \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::reset\fC [private]\fP"

.PP
Indicator if we already trained the model\&. 
.PP
Definition at line 263 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBResetVisitor\fP \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::resetVisitor\fC [private]\fP"

.PP
Locally-stored reset visitor\&. 
.PP
Definition at line 302 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> arma::mat \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::responses\fC [private]\fP"

.PP
The matrix of responses to the input data points\&. 
.PP
Definition at line 275 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> size_t \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::rho\fC [private]\fP"

.PP
Number of steps to backpropagate through time (BPTT)\&. 
.PP
Definition at line 244 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> bool \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::single\fC [private]\fP"

.PP
Only predict the last element of the input sequence\&. 
.PP
Definition at line 266 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> size_t \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::targetSize\fC [private]\fP"

.PP
The target size\&. 
.PP
Definition at line 260 of file rnn\&.hpp\&.
.SS "template<typename OutputLayerType  = NegativeLogLikelihood<>, typename InitializationRuleType  = RandomInitialization> \fBWeightSizeVisitor\fP \fBmlpack::ann::RNN\fP< OutputLayerType, InitializationRuleType >::weightSizeVisitor\fC [private]\fP"

.PP
Locally-stored weight size visitor\&. 
.PP
Definition at line 299 of file rnn\&.hpp\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.
