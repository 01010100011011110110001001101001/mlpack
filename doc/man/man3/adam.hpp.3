.TH "src/mlpack/core/optimizers/adam/adam.hpp" 3 "Sat Mar 25 2017" "Version master" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
src/mlpack/core/optimizers/adam/adam.hpp \- 
.SH SYNOPSIS
.br
.PP
.SS "Classes"

.in +1c
.ti -1c
.RI "class \fBmlpack::optimization::Adam< DecomposableFunctionType >\fP"
.br
.RI "\fI\fBAdam\fP is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients\&. \fP"
.in -1c
.SS "Namespaces"

.in +1c
.ti -1c
.RI " \fBmlpack\fP"
.br
.RI "\fILinear algebra utility functions, generally performed on matrices or vectors\&. \fP"
.ti -1c
.RI " \fBmlpack::optimization\fP"
.br
.in -1c
.SH "Detailed Description"
.PP 

.PP
\fBAuthor:\fP
.RS 4
Ryan Curtin 
.PP
Vasanth Kalingeri 
.PP
Marcus Edel 
.PP
Vivek Pal
.RE
.PP
Adam and AdaMax optimizer\&. Adam is an an algorithm for first-order gradient- -based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments\&. AdaMax is simply a variant of Adam based on the infinity norm\&.
.PP
mlpack is free software; you may redistribute it and/or modify it under the terms of the 3-clause BSD license\&. You should have received a copy of the 3-clause BSD license along with mlpack\&. If not, see http://www.opensource.org/licenses/BSD-3-Clause for more information\&. 
.PP
Definition in file \fBadam\&.hpp\fP\&.
.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.
