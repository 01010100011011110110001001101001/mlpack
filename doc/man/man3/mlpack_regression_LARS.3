.TH "mlpack::regression::LARS" 3 "Sat Mar 25 2017" "Version master" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
mlpack::regression::LARS \- An implementation of \fBLARS\fP, a stage-wise homotopy-based algorithm for l1-regularized linear regression (LASSO) and l1+l2 regularized linear regression (Elastic Net)\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBLARS\fP (const bool \fBuseCholesky\fP=false, const double \fBlambda1\fP=0\&.0, const double \fBlambda2\fP=0\&.0, const double \fBtolerance\fP=1e\-16)"
.br
.RI "\fISet the parameters to \fBLARS\fP\&. \fP"
.ti -1c
.RI "\fBLARS\fP (const bool \fBuseCholesky\fP, const arma::mat &gramMatrix, const double \fBlambda1\fP=0\&.0, const double \fBlambda2\fP=0\&.0, const double \fBtolerance\fP=1e\-16)"
.br
.RI "\fISet the parameters to \fBLARS\fP, and pass in a precalculated Gram matrix\&. \fP"
.ti -1c
.RI "const std::vector< size_t > & \fBActiveSet\fP () const "
.br
.RI "\fIAccess the set of active dimensions\&. \fP"
.ti -1c
.RI "const std::vector< arma::vec > & \fBBetaPath\fP () const "
.br
.RI "\fIAccess the set of coefficients after each iteration; the solution is the last element\&. \fP"
.ti -1c
.RI "const std::vector< double > & \fBLambdaPath\fP () const "
.br
.RI "\fIAccess the set of values for lambda1 after each iteration; the solution is the last element\&. \fP"
.ti -1c
.RI "const arma::mat & \fBMatUtriCholFactor\fP () const "
.br
.RI "\fIAccess the upper triangular cholesky factor\&. \fP"
.ti -1c
.RI "void \fBPredict\fP (const arma::mat &points, arma::vec &predictions, const bool rowMajor=false) const "
.br
.RI "\fIPredict y_i for each data point in the given data matrix, using the currently-trained \fBLARS\fP model (so make sure you run Regress() first)\&. \fP"
.ti -1c
.RI "template<typename Archive > void \fBSerialize\fP (Archive &ar, const unsigned int)"
.br
.RI "\fISerialize the \fBLARS\fP model\&. \fP"
.ti -1c
.RI "void \fBTrain\fP (const arma::mat &data, const arma::vec &responses, arma::vec &beta, const bool transposeData=true)"
.br
.RI "\fIRun \fBLARS\fP\&. \fP"
.in -1c
.SS "Private Member Functions"

.in +1c
.ti -1c
.RI "void \fBActivate\fP (const size_t varInd)"
.br
.RI "\fIAdd dimension varInd to active set\&. \fP"
.ti -1c
.RI "void \fBCholeskyDelete\fP (const size_t colToKill)"
.br
.ti -1c
.RI "void \fBCholeskyInsert\fP (const arma::vec &newX, const arma::mat &X)"
.br
.ti -1c
.RI "void \fBCholeskyInsert\fP (double sqNormNewX, const arma::vec &newGramCol)"
.br
.ti -1c
.RI "void \fBComputeYHatDirection\fP (const arma::mat &matX, const arma::vec &betaDirection, arma::vec &yHatDirection)"
.br
.ti -1c
.RI "void \fBDeactivate\fP (const size_t activeVarInd)"
.br
.RI "\fIRemove activeVarInd'th element from active set\&. \fP"
.ti -1c
.RI "void \fBGivensRotate\fP (const arma::vec::fixed< 2 > &x, arma::vec::fixed< 2 > &rotatedX, arma::mat &G)"
.br
.ti -1c
.RI "void \fBIgnore\fP (const size_t varInd)"
.br
.RI "\fIAdd dimension varInd to ignores set (never removed)\&. \fP"
.ti -1c
.RI "void \fBInterpolateBeta\fP ()"
.br
.in -1c
.SS "Private Attributes"

.in +1c
.ti -1c
.RI "std::vector< size_t > \fBactiveSet\fP"
.br
.RI "\fIActive set of dimensions\&. \fP"
.ti -1c
.RI "std::vector< arma::vec > \fBbetaPath\fP"
.br
.RI "\fISolution path\&. \fP"
.ti -1c
.RI "bool \fBelasticNet\fP"
.br
.RI "\fITrue if this is the elastic net problem\&. \fP"
.ti -1c
.RI "std::vector< size_t > \fBignoreSet\fP"
.br
.RI "\fISet of ignored variables (for dimensions in span{active set dimensions})\&. \fP"
.ti -1c
.RI "std::vector< bool > \fBisActive\fP"
.br
.RI "\fIActive set membership indicator (for each dimension)\&. \fP"
.ti -1c
.RI "std::vector< bool > \fBisIgnored\fP"
.br
.RI "\fIMembership indicator for set of ignored variables\&. \fP"
.ti -1c
.RI "double \fBlambda1\fP"
.br
.RI "\fIRegularization parameter for l1 penalty\&. \fP"
.ti -1c
.RI "double \fBlambda2\fP"
.br
.RI "\fIRegularization parameter for l2 penalty\&. \fP"
.ti -1c
.RI "std::vector< double > \fBlambdaPath\fP"
.br
.RI "\fIValue of lambda_1 for each solution in solution path\&. \fP"
.ti -1c
.RI "bool \fBlasso\fP"
.br
.RI "\fITrue if this is the LASSO problem\&. \fP"
.ti -1c
.RI "const arma::mat * \fBmatGram\fP"
.br
.RI "\fIPointer to the Gram matrix we will use\&. \fP"
.ti -1c
.RI "arma::mat \fBmatGramInternal\fP"
.br
.RI "\fIGram matrix\&. \fP"
.ti -1c
.RI "arma::mat \fBmatUtriCholFactor\fP"
.br
.RI "\fIUpper triangular cholesky factor; initially 0x0 matrix\&. \fP"
.ti -1c
.RI "double \fBtolerance\fP"
.br
.RI "\fITolerance for main loop\&. \fP"
.ti -1c
.RI "bool \fBuseCholesky\fP"
.br
.RI "\fIWhether or not to use Cholesky decomposition when solving linear system\&. \fP"
.in -1c
.SH "Detailed Description"
.PP 
An implementation of \fBLARS\fP, a stage-wise homotopy-based algorithm for l1-regularized linear regression (LASSO) and l1+l2 regularized linear regression (Elastic Net)\&. 

Let $ X $ be a matrix where each row is a point and each column is a dimension and let $ y $ be a vector of responses\&.
.PP
The Elastic Net problem is to solve
.PP
\[ \min_{\beta} 0.5 || X \beta - y ||_2^2 + \lambda_1 || \beta ||_1 + 0.5 \lambda_2 || \beta ||_2^2 \].PP
where $ \beta $ is the vector of regression coefficients\&.
.PP
If $ \lambda_1 > 0 $ and $ \lambda_2 = 0 $, the problem is the LASSO\&. If $ \lambda_1 > 0 $ and $ \lambda_2 > 0 $, the problem is the elastic net\&. If $ \lambda_1 = 0 $ and $ \lambda_2 > 0 $, the problem is ridge regression\&. If $ \lambda_1 = 0 $ and $ \lambda_2 = 0 $, the problem is unregularized linear regression\&.
.PP
Note: This algorithm is not recommended for use (in terms of efficiency) when $ \lambda_1 $ = 0\&.
.PP
For more details, see the following papers:
.PP
.PP
.nf
@article{efron2004least,
  title={Least angle regression},
  author={Efron, B\&. and Hastie, T\&. and Johnstone, I\&. and Tibshirani, R\&.},
  journal={The Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}
.fi
.PP
.PP
.PP
.nf
@article{zou2005regularization,
  title={Regularization and variable selection via the elastic net},
  author={Zou, H\&. and Hastie, T\&.},
  journal={Journal of the Royal Statistical Society Series B},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Royal Statistical Society}
}
.fi
.PP
 
.PP
Definition at line 89 of file lars\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "mlpack::regression::LARS::LARS (const bool useCholesky = \fCfalse\fP, const double lambda1 = \fC0\&.0\fP, const double lambda2 = \fC0\&.0\fP, const double tolerance = \fC1e\-16\fP)"

.PP
Set the parameters to \fBLARS\fP\&. Both lambda1 and lambda2 default to 0\&.
.PP
\fBParameters:\fP
.RS 4
\fIuseCholesky\fP Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix)\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fItolerance\fP Run until the maximum correlation of elements in (X^T y) is less than this\&. 
.RE
.PP

.SS "mlpack::regression::LARS::LARS (const bool useCholesky, const arma::mat & gramMatrix, const double lambda1 = \fC0\&.0\fP, const double lambda2 = \fC0\&.0\fP, const double tolerance = \fC1e\-16\fP)"

.PP
Set the parameters to \fBLARS\fP, and pass in a precalculated Gram matrix\&. Both lambda1 and lambda2 default to 0\&.
.PP
\fBParameters:\fP
.RS 4
\fIuseCholesky\fP Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix)\&. 
.br
\fIgramMatrix\fP Gram matrix\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fItolerance\fP Run until the maximum correlation of elements in (X^T y) is less than this\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "void mlpack::regression::LARS::Activate (const size_t varInd)\fC [private]\fP"

.PP
Add dimension varInd to active set\&. 
.PP
\fBParameters:\fP
.RS 4
\fIvarInd\fP Dimension to add to active set\&. 
.RE
.PP

.SS "const std::vector<size_t>& mlpack::regression::LARS::ActiveSet () const\fC [inline]\fP"

.PP
Access the set of active dimensions\&. 
.PP
Definition at line 158 of file lars\&.hpp\&.
.PP
References activeSet\&.
.SS "const std::vector<arma::vec>& mlpack::regression::LARS::BetaPath () const\fC [inline]\fP"

.PP
Access the set of coefficients after each iteration; the solution is the last element\&. 
.PP
Definition at line 162 of file lars\&.hpp\&.
.PP
References betaPath\&.
.SS "void mlpack::regression::LARS::CholeskyDelete (const size_t colToKill)\fC [private]\fP"

.SS "void mlpack::regression::LARS::CholeskyInsert (const arma::vec & newX, const arma::mat & X)\fC [private]\fP"

.SS "void mlpack::regression::LARS::CholeskyInsert (double sqNormNewX, const arma::vec & newGramCol)\fC [private]\fP"

.SS "void mlpack::regression::LARS::ComputeYHatDirection (const arma::mat & matX, const arma::vec & betaDirection, arma::vec & yHatDirection)\fC [private]\fP"

.SS "void mlpack::regression::LARS::Deactivate (const size_t activeVarInd)\fC [private]\fP"

.PP
Remove activeVarInd'th element from active set\&. 
.PP
\fBParameters:\fP
.RS 4
\fIactiveVarInd\fP Index of element to remove from active set\&. 
.RE
.PP

.SS "void mlpack::regression::LARS::GivensRotate (const arma::vec::fixed< 2 > & x, arma::vec::fixed< 2 > & rotatedX, arma::mat & G)\fC [private]\fP"

.SS "void mlpack::regression::LARS::Ignore (const size_t varInd)\fC [private]\fP"

.PP
Add dimension varInd to ignores set (never removed)\&. 
.PP
\fBParameters:\fP
.RS 4
\fIvarInd\fP Dimension to add to ignores set\&. 
.RE
.PP

.SS "void mlpack::regression::LARS::InterpolateBeta ()\fC [private]\fP"

.SS "const std::vector<double>& mlpack::regression::LARS::LambdaPath () const\fC [inline]\fP"

.PP
Access the set of values for lambda1 after each iteration; the solution is the last element\&. 
.PP
Definition at line 166 of file lars\&.hpp\&.
.PP
References lambdaPath\&.
.SS "const arma::mat& mlpack::regression::LARS::MatUtriCholFactor () const\fC [inline]\fP"

.PP
Access the upper triangular cholesky factor\&. 
.PP
Definition at line 169 of file lars\&.hpp\&.
.PP
References matUtriCholFactor, and Serialize()\&.
.SS "void mlpack::regression::LARS::Predict (const arma::mat & points, arma::vec & predictions, const bool rowMajor = \fCfalse\fP) const"

.PP
Predict y_i for each data point in the given data matrix, using the currently-trained \fBLARS\fP model (so make sure you run Regress() first)\&. If the data matrix is row-major (as opposed to the usual column-major format for mlpack matrices), set rowMajor = true to avoid an extra transpose\&.
.PP
\fBParameters:\fP
.RS 4
\fIpoints\fP The data points to regress on\&. 
.br
\fIpredictions\fP y, which will contained calculated values on completion\&. 
.RE
.PP

.SS "template<typename Archive > void mlpack::regression::LARS::Serialize (Archive & ar, const unsigned int)"

.PP
Serialize the \fBLARS\fP model\&. 
.PP
Referenced by MatUtriCholFactor()\&.
.SS "void mlpack::regression::LARS::Train (const arma::mat & data, const arma::vec & responses, arma::vec & beta, const bool transposeData = \fCtrue\fP)"

.PP
Run \fBLARS\fP\&. The input matrix (like all mlpack matrices) should be column-major -- each column is an observation and each row is a dimension\&. However, because \fBLARS\fP is more efficient on a row-major matrix, this method will (internally) transpose the matrix\&. If this transposition is not necessary (i\&.e\&., you want to pass in a row-major matrix), pass 'false' for the transposeData parameter\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Column-major input data (or row-major input data if rowMajor = true)\&. 
.br
\fIresponses\fP A vector of targets\&. 
.br
\fIbeta\fP Vector to store the solution (the coefficients) in\&. 
.br
\fItransposeData\fP Set to false if the data is row-major\&. 
.RE
.PP

.SH "Member Data Documentation"
.PP 
.SS "std::vector<size_t> mlpack::regression::LARS::activeSet\fC [private]\fP"

.PP
Active set of dimensions\&. 
.PP
Definition at line 210 of file lars\&.hpp\&.
.PP
Referenced by ActiveSet()\&.
.SS "std::vector<arma::vec> mlpack::regression::LARS::betaPath\fC [private]\fP"

.PP
Solution path\&. 
.PP
Definition at line 204 of file lars\&.hpp\&.
.PP
Referenced by BetaPath()\&.
.SS "bool mlpack::regression::LARS::elasticNet\fC [private]\fP"

.PP
True if this is the elastic net problem\&. 
.PP
Definition at line 196 of file lars\&.hpp\&.
.SS "std::vector<size_t> mlpack::regression::LARS::ignoreSet\fC [private]\fP"

.PP
Set of ignored variables (for dimensions in span{active set dimensions})\&. 
.PP
Definition at line 218 of file lars\&.hpp\&.
.SS "std::vector<bool> mlpack::regression::LARS::isActive\fC [private]\fP"

.PP
Active set membership indicator (for each dimension)\&. 
.PP
Definition at line 213 of file lars\&.hpp\&.
.SS "std::vector<bool> mlpack::regression::LARS::isIgnored\fC [private]\fP"

.PP
Membership indicator for set of ignored variables\&. 
.PP
Definition at line 221 of file lars\&.hpp\&.
.SS "double mlpack::regression::LARS::lambda1\fC [private]\fP"

.PP
Regularization parameter for l1 penalty\&. 
.PP
Definition at line 193 of file lars\&.hpp\&.
.SS "double mlpack::regression::LARS::lambda2\fC [private]\fP"

.PP
Regularization parameter for l2 penalty\&. 
.PP
Definition at line 198 of file lars\&.hpp\&.
.SS "std::vector<double> mlpack::regression::LARS::lambdaPath\fC [private]\fP"

.PP
Value of lambda_1 for each solution in solution path\&. 
.PP
Definition at line 207 of file lars\&.hpp\&.
.PP
Referenced by LambdaPath()\&.
.SS "bool mlpack::regression::LARS::lasso\fC [private]\fP"

.PP
True if this is the LASSO problem\&. 
.PP
Definition at line 191 of file lars\&.hpp\&.
.SS "const arma::mat* mlpack::regression::LARS::matGram\fC [private]\fP"

.PP
Pointer to the Gram matrix we will use\&. 
.PP
Definition at line 182 of file lars\&.hpp\&.
.SS "arma::mat mlpack::regression::LARS::matGramInternal\fC [private]\fP"

.PP
Gram matrix\&. 
.PP
Definition at line 179 of file lars\&.hpp\&.
.SS "arma::mat mlpack::regression::LARS::matUtriCholFactor\fC [private]\fP"

.PP
Upper triangular cholesky factor; initially 0x0 matrix\&. 
.PP
Definition at line 185 of file lars\&.hpp\&.
.PP
Referenced by MatUtriCholFactor()\&.
.SS "double mlpack::regression::LARS::tolerance\fC [private]\fP"

.PP
Tolerance for main loop\&. 
.PP
Definition at line 201 of file lars\&.hpp\&.
.SS "bool mlpack::regression::LARS::useCholesky\fC [private]\fP"

.PP
Whether or not to use Cholesky decomposition when solving linear system\&. 
.PP
Definition at line 188 of file lars\&.hpp\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.
